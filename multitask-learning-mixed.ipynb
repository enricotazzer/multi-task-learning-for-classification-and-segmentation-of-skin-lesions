{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.13.1"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13894940,"sourceType":"datasetVersion","datasetId":8852482}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Multi-Task Skin Lesion Diagnostic\nThis notebook sketches a PyTorch implementation of the architecture described in *Multi-Task Classification and Segmentation for Explicable Capsule Endoscopy Diagnostics*. The model shares an encoder across tasks and uses separate heads for frame-level classification and pixel-level lesion segmentation.","metadata":{}},{"cell_type":"code","source":"# Core libraries, torch modules, and torchvision utilities used throughout the notebook\nfrom contextlib import nullcontext\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Dict, Iterator, List, Optional, Tuple\n\nimport random\n\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.cuda.amp import GradScaler, autocast\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\nfrom torchvision.models import ResNet50_Weights, resnet50\nfrom torchvision.transforms import InterpolationMode\nimport torchvision.transforms.functional as TF\n\n# Fix random seed so the data augmentations and weight initialisation remain reproducible\ntorch.manual_seed(42)\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model configurations","metadata":{}},{"cell_type":"code","source":"# Configuration\n@dataclass\nclass Config:\n    \"\"\"Central hyper-parameters mirroring the settings reported in the paper.\"\"\"\n    project_root: Path = Path(\"/kaggle/input/skin-cancer-multitask-learning-mixed-data\")\n    mixed_dataset_name: str = \"multitasked_dataset_mixed\"\n    image_size: Tuple[int, int] = (256, 256)  # input resolution for both tasks\n    batch_size: int = 8  # classification mini-batch size\n    segmentation_batch_size: int = 4  # segmentation mini-batch size (decoder is heavier)\n    base_learning_rate: float = 2e-4\n    weight_decay: float = 1e-4\n    max_epochs: int = 20\n    alpha: float = 0.1  # fusion-penalty weight α used in the paper for regularisation\n    classification_loss_weight: float = 0.5  # balance between classification and segmentation losses\n    segmentation_loss_weight: float = 0.5\n    class_names: Tuple[str, ...] = (\"MEL\", \"NV\", \"BCC\", \"AKIEC\", \"BKL\", \"DF\", \"VASC\")\n    ignore_index: int = 255  # optional label to mask out invalid pixels\n    min_learning_rate: float = 1e-5\n    scheduler_period: int = 10  # cosine-annealing period (epochs)\n    mixed_precision: bool = True  # enable AMP when CUDA is available\n    segmentation_positive_weight: float = 1.5  # kept for potential focal weighting experiments\n    grad_clip_norm: Optional[float] = 5.0  # guard against exploding gradients\n    imagenet_mean: Tuple[float, float, float] = (0.485, 0.456, 0.406)\n    imagenet_std: Tuple[float, float, float] = (0.229, 0.224, 0.225)\n    segmentation_suffix: str = \"_segmentation\"\n    mixed_dataset_root: Optional[Path] = None\n\n    def __post_init__(self) -> None:\n        if self.mixed_dataset_root is None:\n            object.__setattr__(self, \"mixed_dataset_root\", self.project_root / self.mixed_dataset_name)\n        if not self.mixed_dataset_root.exists():\n            raise FileNotFoundError(\n                f\"Expected mixed dataset directory at {self.mixed_dataset_root}, but it was not found.\"\n            )\n\n    def manifest_path(self, subset: str) -> Path:\n        \"\"\"Return the manifest CSV for a given subset inside the mixed dataset.\"\"\"\n        path = self.mixed_dataset_root / subset / \"manifest.csv\"\n        if not path.exists():\n            raise FileNotFoundError(f\"Manifest for subset '{subset}' not found at {path}\")\n        return path\n\n    def resolve_mixed_path(self, relative_path: str) -> Path:\n        \"\"\"Resolve a manifest path (stored relative to the mixed dataset root) to an absolute Path.\"\"\"\n        abs_path = self.mixed_dataset_root / Path(relative_path)\n        if not abs_path.exists():\n            raise FileNotFoundError(f\"Resolved path does not exist: {abs_path}\")\n        return abs_path\n    \ncfg = Config()\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Image utilites","metadata":{}},{"cell_type":"code","source":"# Data transforms\ndef build_classification_transform(cfg: Config, train: bool) -> transforms.Compose:\n    \"\"\"Compose the augmentations used for the image-level classifier.\"\"\"\n    augmentations: List[transforms.Compose]\n    if train:\n        augmentations = [\n            # Random crops and flips mimic the appearance variability described in the paper\n            transforms.RandomResizedCrop(cfg.image_size, scale=(0.8, 1.0), interpolation=InterpolationMode.BILINEAR),\n            transforms.RandomHorizontalFlip(p=0.5),\n            transforms.RandomVerticalFlip(p=0.1),\n            transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n        ]\n    else:\n        augmentations = [\n            transforms.Resize(cfg.image_size, interpolation=InterpolationMode.BILINEAR),\n            transforms.CenterCrop(cfg.image_size),\n        ]\n    augmentations += [\n        transforms.ToTensor(),\n        transforms.Normalize(mean=cfg.imagenet_mean, std=cfg.imagenet_std),\n    ]\n    return transforms.Compose(augmentations)\n\n\ndef apply_segmentation_transforms(\n    image: Image.Image, mask: Image.Image, cfg: Config, train: bool\n) -> Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Apply the paper's spatial augmentations jointly and emit integer masks for cross-entropy.\"\"\"\n    image = image.convert(\"RGB\")\n    mask = mask.convert(\"L\")\n    if train:\n        if random.random() < 0.5:\n            image = TF.hflip(image)\n            mask = TF.hflip(mask)\n        if random.random() < 0.2:\n            image = TF.vflip(image)\n            mask = TF.vflip(mask)\n        if random.random() < 0.3:\n            angle = random.uniform(-15.0, 15.0)\n            image = TF.rotate(image, angle, interpolation=InterpolationMode.BILINEAR)\n            mask = TF.rotate(mask, angle, interpolation=InterpolationMode.NEAREST)\n    image = TF.resize(image, cfg.image_size, interpolation=InterpolationMode.BILINEAR)\n    mask = TF.resize(mask, cfg.image_size, interpolation=InterpolationMode.NEAREST)\n    image_tensor = TF.normalize(TF.to_tensor(image), mean=cfg.imagenet_mean, std=cfg.imagenet_std)\n    # Binarise lesion area then convert to class indices (0 background, 1 lesion) for pixel-wise cross-entropy\n    mask_array = (np.array(mask, dtype=np.uint8) > 0).astype(np.int64)\n    mask_tensor = torch.from_numpy(mask_array)\n    return image_tensor, mask_tensor","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dataset ","metadata":{}},{"cell_type":"code","source":"# Dataset definitions\nclass ISICClassificationDataset(Dataset):\n    \"\"\"Loads image-level labels used for the global diagnostic head.\"\"\"\n\n    def __init__(self, cfg: Config, split: str):\n        self.cfg = cfg\n        self.split = split\n        self.transform = build_classification_transform(self.cfg, self.split == \"train\")\n        label_columns = list(cfg.class_names)\n        manifests: List[pd.DataFrame] = []\n\n        for subset in (\"paired\", \"classification_only\"):\n            manifest_path = cfg.manifest_path(subset)\n            subset_df = pd.read_csv(manifest_path)\n            if \"split\" not in subset_df.columns:\n                raise KeyError(f\"Manifest at {manifest_path} is missing a 'split' column\")\n            subset_df = subset_df[subset_df[\"split\"] == split].copy()\n            if subset_df.empty:\n                continue\n            subset_df[\"image_abs\"] = subset_df[\"image\"].apply(lambda rel: str(cfg.resolve_mixed_path(rel)))\n            subset_df[\"image_id\"] = subset_df[\"image\"].apply(lambda rel: Path(rel).stem)\n            manifests.append(subset_df)\n\n        if not manifests:\n            raise RuntimeError(f\"No classification samples found for split '{split}' in the mixed dataset\")\n\n        self.metadata = pd.concat(manifests, ignore_index=True)\n        missing_cols = [col for col in label_columns if col not in self.metadata.columns]\n        if missing_cols:\n            raise RuntimeError(\n                \"Classification manifest is missing expected label columns: \" + \", \".join(missing_cols)\n            )\n        self.label_vectors = self.metadata[label_columns].values.astype(np.float32)\n        self.image_paths = [Path(path) for path in self.metadata[\"image_abs\"]]\n        self.image_ids = self.metadata[\"image_id\"].tolist()\n\n    def __len__(self) -> int:\n        return len(self.image_paths)\n\n    def __getitem__(self, index: int) -> Dict[str, torch.Tensor]:\n        image_path = self.image_paths[index]\n        if not image_path.exists():\n            raise FileNotFoundError(f\"Classification image not found at {image_path}\")\n        image = Image.open(image_path)\n        tensor = self.transform(image)\n        label_vector = self.label_vectors[index]\n        label = torch.tensor(int(label_vector.argmax()), dtype=torch.long)\n        return {\n            \"image\": tensor,\n            \"label\": label,\n            \"label_one_hot\": torch.from_numpy(label_vector),\n            \"image_id\": self.image_ids[index],\n        }\n\n\nclass ISICSegmentationDataset(Dataset):\n    \"\"\"Provides pixel-level annotations consumed by the decoder/fusion branches.\"\"\"\n\n    def __init__(self, cfg: Config, split: str):\n        self.cfg = cfg\n        self.split = split\n        manifests: List[pd.DataFrame] = []\n\n        for subset in (\"paired\", \"segmentation_only\"):\n            manifest_path = cfg.manifest_path(subset)\n            subset_df = pd.read_csv(manifest_path)\n            if \"split\" not in subset_df.columns:\n                raise KeyError(f\"Manifest at {manifest_path} is missing a 'split' column\")\n            subset_df = subset_df[subset_df[\"split\"] == split].copy()\n            if subset_df.empty:\n                continue\n            subset_df[\"image_abs\"] = subset_df[\"image\"].apply(lambda rel: str(cfg.resolve_mixed_path(rel)))\n            subset_df[\"mask_abs\"] = subset_df[\"mask\"].apply(lambda rel: str(cfg.resolve_mixed_path(rel)))\n            subset_df[\"image_id\"] = subset_df[\"image\"].apply(lambda rel: Path(rel).stem)\n            manifests.append(subset_df)\n\n        if not manifests:\n            raise RuntimeError(f\"No segmentation samples found for split '{split}' in the mixed dataset\")\n\n        self.metadata = pd.concat(manifests, ignore_index=True)\n        self.image_paths = [Path(path) for path in self.metadata[\"image_abs\"]]\n        self.mask_paths = [Path(path) for path in self.metadata[\"mask_abs\"]]\n        self.image_ids = self.metadata[\"image_id\"].tolist()\n\n    def __len__(self) -> int:\n        return len(self.image_paths)\n\n    def __getitem__(self, index: int) -> Dict[str, torch.Tensor]:\n        image_path = self.image_paths[index]\n        mask_path = self.mask_paths[index]\n        if not image_path.exists():\n            raise FileNotFoundError(f\"Segmentation image not found at {image_path}\")\n        if not mask_path.exists():\n            raise FileNotFoundError(f\"Segmentation mask not found at {mask_path}\")\n        image = Image.open(image_path)\n        mask = Image.open(mask_path)\n        image_tensor, mask_tensor = apply_segmentation_transforms(\n            image, mask, self.cfg, train=self.split == \"train\"\n        )\n        return {\n            \"image\": image_tensor,\n            \"mask\": mask_tensor,\n            \"image_id\": self.image_ids[index],\n        }","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# DataModule-style helpers\ndef create_datasets(cfg: Config) -> Tuple[Dict[str, Dataset], Dict[str, Dataset]]:\n    \"\"\"Instantiate train/val/test splits for both tasks.\"\"\"\n    classification = {split: ISICClassificationDataset(cfg, split) for split in (\"train\", \"val\", \"test\")}\n    segmentation = {split: ISICSegmentationDataset(cfg, split) for split in (\"train\", \"val\", \"test\")}\n    return classification, segmentation\n\n\ndef create_dataloaders(\n    cfg: Config,\n    classification: Dict[str, Dataset],\n    segmentation: Dict[str, Dataset],\n) -> Tuple[Dict[str, DataLoader], Dict[str, DataLoader]]:\n    \"\"\"Wrap datasets in PyTorch dataloaders with paper-inspired batching.\"\"\"\n    classification_loaders = {\n        split: DataLoader(\n            dataset,\n            batch_size=cfg.batch_size,\n            shuffle=split == \"train\",\n            drop_last=split == \"train\"\n        )\n        for split, dataset in classification.items()\n    }\n    segmentation_loaders = {\n        split: DataLoader(\n            dataset,\n            batch_size=cfg.segmentation_batch_size,\n            shuffle=split == \"train\",\n            drop_last=split == \"train\"\n        )\n        for split, dataset in segmentation.items()\n    }\n    return classification_loaders, segmentation_loaders\n\nclassification, segmentation = create_datasets(cfg)\nclassification_loader, segmentation_loader = create_dataloaders(cfg, classification, segmentation) \n# loaders are dict of the form {train: train_loader, val: val_loader, test: test_loader}\n# The print acts as a quick sanity check mirroring dataset stats shown in the paper\nprint(\"Loaders created succesfully\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Dataset sanity check","metadata":{}},{"cell_type":"code","source":"# Quick dataset sanity check\nclassification_datasets, segmentation_datasets = create_datasets(cfg)\n# Mirror the counts reported in the appendix (ensures file structure is correct)\nprint({split: len(ds) for split, ds in classification_datasets.items()})\nprint({split: len(ds) for split, ds in segmentation_datasets.items()})\nsample_cls = classification_datasets[\"train\"][0]\nsample_seg = segmentation_datasets[\"train\"][0]\nprint(\"Classification sample:\", sample_cls[\"image\"].shape, sample_cls[\"label_one_hot\"], sample_cls[\"image_id\"])\nprint(\"Segmentation sample:\", sample_seg[\"image\"].shape, sample_seg[\"mask\"].shape, sample_seg[\"image_id\"])","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model definition","metadata":{}},{"cell_type":"code","source":"# Multi-task model skeleton with ResNet-50 backbone, dual heads, and stacked cross-fusion modules\nclass ConvBNReLU(nn.Module):\n    \"\"\"Utility block repeatedly used in decoder refinements (convolution + BN + ReLU).\"\"\"\n\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int = 3, stride: int = 1):\n        super().__init__()  # register parameters and buffers\n        padding = kernel_size // 2  # preserve spatial size for odd kernels to keep skip connections aligned\n        self.block = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=False),\n            nn.BatchNorm2d(out_channels),  # stabilise feature statistics while training\n            nn.ReLU(inplace=True),  # lightweight non-linearity reusing storage\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.block(x)\n\n\nclass ClassificationBranch(nn.Module):\n    \"\"\"Fully connected classifier with four hidden layers, dropout regularisation, and logits head.\"\"\"\n\n    def __init__(self, in_features: int, hidden_dims: Tuple[int, int, int, int], num_classes: int):\n        super().__init__()\n        if len(hidden_dims) != 4:\n            raise ValueError(\"hidden_dims must contain four entries for the extended branch\")\n        self.hidden_dims = hidden_dims  # expose dims for cross-fusion wiring\n        dims = (in_features,) + hidden_dims\n        # Construct sequential linear layers that can be selectively invoked after each fusion stage\n        self.layers = nn.ModuleList(\n            nn.Linear(dims[i], dims[i + 1]) for i in range(len(hidden_dims))\n        )\n        self.dropout = nn.Dropout(p=0.3)  # regularise intermediate representations\n        self.head = nn.Linear(hidden_dims[-1], num_classes)  # final logits for frame-level classification\n\n    def activate_layer(self, x: torch.Tensor, idx: int) -> torch.Tensor:\n        \"\"\"Apply the idx-th hidden layer followed by ReLU and dropout.\"\"\"\n        if idx < 0 or idx >= len(self.layers):\n            raise IndexError(\"Layer index out of range for classification branch\")\n        x = self.layers[idx](x)\n        x = F.relu(x)\n        return self.dropout(x)\n\n    def final_logits(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"Compute classification logits from the last hidden representation.\"\"\"\n        return self.head(x)\n\n\nclass SegmentationBranch(nn.Module):\n    \"\"\"Image-to-image decoder with additional refinement blocks for deeper fusion points.\"\"\"\n\n    def __init__(self, encoder_channels: Tuple[int, int, int, int, int], out_channels: int):\n        super().__init__()\n        c0, c1, c2, c3, c4 = encoder_channels\n        # Standard U-Net style upsampling path: progressively merge encoder features with decoder activations\n        self.reduce = ConvBNReLU(c4, 512, kernel_size=1)  # compress deepest features before upsampling\n        self.up3 = ConvBNReLU(512 + c3, 256)\n        self.up2 = ConvBNReLU(256 + c2, 128)\n        self.up1 = ConvBNReLU(128 + c1, 96)\n        self.up0 = ConvBNReLU(96 + c0, 64)\n        # Refinement convolutions let fusion interactions happen on stable feature maps\n        self.refine1 = ConvBNReLU(64, 64)  # refinement prior to second fusion\n        self.refine2 = ConvBNReLU(64, 64)  # refinement prior to prediction\n        # Produce background/lesion logits so a softmax classifier can score each pixel\n        self.prediction = nn.Conv2d(64, out_channels, kernel_size=1)\n\n    def decode(self, features: Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]) -> torch.Tensor:\n        \"\"\"Run the entire top-down decoder and return the high-resolution feature map for fusion.\"\"\"\n        c0, c1, c2, c3, c4 = features\n        x = self.reduce(c4)\n        x = F.interpolate(x, size=c3.shape[-2:], mode=\"bilinear\", align_corners=False)\n        x = self.up3(torch.cat([x, c3], dim=1))\n        x = F.interpolate(x, size=c2.shape[-2:], mode=\"bilinear\", align_corners=False)\n        x = self.up2(torch.cat([x, c2], dim=1))\n        x = F.interpolate(x, size=c1.shape[-2:], mode=\"bilinear\", align_corners=False)\n        x = self.up1(torch.cat([x, c1], dim=1))\n        x = F.interpolate(x, size=c0.shape[-2:], mode=\"bilinear\", align_corners=False)\n        return self.up0(torch.cat([x, c0], dim=1))  # high-res feature map used for fusion\n\n    def predict(self, features: torch.Tensor) -> torch.Tensor:\n        \"\"\"Project refined features to per-pixel logits (softmax later gives class probabilities).\"\"\"\n        return self.prediction(features)\n\n\nclass CrossFusionModule(nn.Module):\n    \"\"\"Bidirectional cross-fusion: couples classification (global) and segmentation (dense) features.\"\"\"\n\n    def __init__(self, cls_channels: int, seg_channels: int):\n        super().__init__()\n        # Shared transformation matrix M from the paper (Eq. 7) implemented as a 1×1 convolution\n        self.transform = nn.Conv2d(seg_channels, cls_channels, kernel_size=1, bias=False)\n        self.pool = nn.AdaptiveAvgPool2d(1)  # used in the segmentation→classification path\n\n    def forward(\n        self,\n        cls_feature: torch.Tensor,  # shape (batch, cls_channels, 1, 1)\n        seg_feature: torch.Tensor,  # shape (batch, seg_channels, H, W)\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        # X'_cls = X_cls + Pool(M(X_seg)) — Eq. (7a) converts dense activations to a global descriptor\n        seg_to_cls = self.pool(self.transform(seg_feature))\n        fused_cls = cls_feature + seg_to_cls\n\n        # X'_seg = X_seg + M^T(Pad(X_cls)) — Eq. (7b) injects classification context into the decoder\n        h, w = seg_feature.shape[-2:]\n        cls_expanded = cls_feature.expand(-1, cls_feature.size(1), h, w)  # Pad(X_cls) for spatial compatibility\n        cls_to_seg = F.conv_transpose2d(cls_expanded, self.transform.weight)  # transpose convolution applies M^T\n        fused_seg = seg_feature + cls_to_seg\n        return fused_cls, fused_seg\n\n\nclass MultiTask(nn.Module):\n    \"\"\"Multi-task architecture with shared ResNet-50 encoder and stacked cross-fusion heads.\"\"\"\n\n    def __init__(\n        self,\n        num_classes: int,\n        num_segmentation_classes: int = 2,\n        trainable_backbone_layers: int = 3,\n        use_pretrained: bool = True,\n    ):\n        super().__init__()\n        weights = ResNet50_Weights.DEFAULT if use_pretrained else None\n        backbone = resnet50(weights=weights)\n\n        # Expose ResNet-50 feature stages for skip connections and fine-tuning control\n        self.stem = nn.Sequential(backbone.conv1, backbone.bn1, backbone.relu)\n        self.pool0 = backbone.maxpool\n        self.layer1 = backbone.layer1\n        self.layer2 = backbone.layer2\n        self.layer3 = backbone.layer3\n        self.layer4 = backbone.layer4\n        self._set_trainable_layers(trainable_backbone_layers)\n\n        encoder_channels = (64, 256, 512, 1024, 2048)  # channel counts produced by each ResNet stage\n        classification_hidden_dims = (1024, 512, 256, 128)  # width of the four FC layers\n        self.classifier_pool = nn.AdaptiveAvgPool2d(1)  # compress encoder output before the MLP\n        self.classifier_branch = ClassificationBranch(\n            in_features=encoder_channels[-1], hidden_dims=classification_hidden_dims, num_classes=num_classes\n        )\n        self.segmentation_branch = SegmentationBranch(\n            encoder_channels=encoder_channels, out_channels=num_segmentation_classes\n        )\n\n        # Two cross-fusion modules placed between consecutive layers of each branch per revised spec\n        self.cross_fusion_primary = CrossFusionModule(\n            cls_channels=self.classifier_branch.hidden_dims[0], seg_channels=64\n        )\n        self.cross_fusion_secondary = CrossFusionModule(\n            cls_channels=self.classifier_branch.hidden_dims[2], seg_channels=64\n        )\n\n    def _set_trainable_layers(self, trainable_backbone_layers: int) -> None:\n        \"\"\"Freeze early ResNet blocks to control how much of the backbone is fine-tuned.\"\"\"\n        stages = [\n            self.stem,\n            self.pool0,\n            self.layer1,\n            self.layer2,\n            self.layer3,\n            self.layer4,\n        ]\n        if trainable_backbone_layers < 1 or trainable_backbone_layers > len(stages):\n            raise ValueError(\n                f\"trainable_backbone_layers must be between 1 and {len(stages)}; got {trainable_backbone_layers}\"\n            )\n        trainable_modules = set(stages[-trainable_backbone_layers:])\n        for module in stages:\n            for param in module.parameters():\n                param.requires_grad = module in trainable_modules\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        task: Optional[str] = None,\n    ) -> Dict[str, torch.Tensor]:\n        \"\"\"Run shared encoder, two-stage cross-fusion, and emit requested task predictions.\"\"\"\n        if task and task not in {\"classification\", \"segmentation\"}:\n            raise ValueError(\"task must be 'classification', 'segmentation', or None\")\n\n        # Shared ResNet-50 encoder produces multi-scale features for both tasks\n        c0 = self.stem(x)\n        p0 = self.pool0(c0)\n        c1 = self.layer1(p0)\n        c2 = self.layer2(c1)\n        c3 = self.layer3(c2)\n        c4 = self.layer4(c3)\n        encoder_features = (c0, c1, c2, c3, c4)\n\n        # First branch layers\n        seg_stage0 = self.segmentation_branch.decode(encoder_features)  # segmentation layer 1\n        cls_vector0 = self.classifier_pool(c4).flatten(1)  # base embedding for the classifier\n        cls_hidden1 = self.classifier_branch.activate_layer(cls_vector0, idx=0)  # classification layer 1\n        cls_hidden1_map = cls_hidden1.view(cls_hidden1.size(0), cls_hidden1.size(1), 1, 1)\n\n        fusion_penalties: List[torch.Tensor] = []\n\n        # Cross-fusion between first and second layers\n        cls_fused1, seg_fused1 = self.cross_fusion_primary(cls_hidden1_map, seg_stage0)\n        fusion_penalties.append(F.mse_loss(cls_fused1, cls_hidden1_map, reduction=\"mean\"))\n        fusion_penalties.append(F.mse_loss(seg_fused1, seg_stage0, reduction=\"mean\"))\n\n        cls_stage1 = cls_fused1.flatten(1)\n        cls_hidden2 = self.classifier_branch.activate_layer(cls_stage1, idx=1)  # classification layer 2\n        seg_stage1 = self.segmentation_branch.refine1(seg_fused1)  # segmentation layer 2\n\n        # Progress to third layers of both branches\n        cls_hidden3 = self.classifier_branch.activate_layer(cls_hidden2, idx=2)  # classification layer 3\n        cls_hidden3_map = cls_hidden3.view(cls_hidden3.size(0), cls_hidden3.size(1), 1, 1)\n        seg_stage2 = self.segmentation_branch.refine2(seg_stage1)  # segmentation layer 3\n\n        # Cross-fusion between third and final layers\n        cls_fused2, seg_fused2 = self.cross_fusion_secondary(cls_hidden3_map, seg_stage2)\n        fusion_penalties.append(F.mse_loss(cls_fused2, cls_hidden3_map, reduction=\"mean\"))\n        fusion_penalties.append(F.mse_loss(seg_fused2, seg_stage2, reduction=\"mean\"))\n\n        cls_stage3 = cls_fused2.flatten(1)\n        cls_hidden4 = self.classifier_branch.activate_layer(cls_stage3, idx=3)  # classification layer 4\n        cls_logits = self.classifier_branch.final_logits(cls_hidden4)\n\n        seg_logits = self.segmentation_branch.predict(seg_fused2)  # segmentation final layer\n        seg_logits = F.interpolate(seg_logits, size=x.shape[-2:], mode=\"bilinear\", align_corners=False)\n\n        fusion_reg = torch.stack(fusion_penalties).sum()\n\n        outputs: Dict[str, torch.Tensor] = {}\n        requested = {task} if task else {\"classification\", \"segmentation\"}\n        if \"classification\" in requested:\n            outputs[\"classification\"] = cls_logits\n            outputs[\"fusion_reg\"] = fusion_reg\n        if \"segmentation\" in requested:\n            outputs[\"segmentation\"] = seg_logits\n        return outputs\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Losses and metrics\ndef build_loss_functions(cfg: Config, device: torch.device) -> Tuple[nn.Module, nn.Module]:\n    \"\"\"Prepare task-specific objectives used during optimisation.\"\"\"\n    # Classification: standard cross-entropy over lesion categories\n    classification_loss = nn.CrossEntropyLoss()\n    # Segmentation: pixel-wise cross-entropy between softmax logits and ground-truth class map\n    segmentation_loss = nn.CrossEntropyLoss(ignore_index=cfg.ignore_index)\n    return classification_loss, segmentation_loss\n\n\ndef classification_accuracy(logits: torch.Tensor, labels: torch.Tensor) -> float:\n    \"\"\"Compute top-1 accuracy for the classification branch.\"\"\"\n    predictions = logits.argmax(dim=1)\n    correct = (predictions == labels).sum().item()\n    total = labels.numel()\n    return correct / max(total, 1)\n\n\ndef dice_score(logits: torch.Tensor, targets: torch.Tensor, threshold: float = 0.5, eps: float = 1e-6) -> float:\n    \"\"\"Compute Dice overlap for the lesion class using softmax probabilities.\"\"\"\n    probabilities = torch.softmax(logits, dim=1)[:, 1, ...]  # lesion channel\n    preds = (probabilities > threshold).float()\n    targets = targets.float()\n    intersection = (preds * targets).sum(dim=(1, 2))\n    union = preds.sum(dim=(1, 2)) + targets.sum(dim=(1, 2))\n    dice = (2 * intersection + eps) / (union + eps)\n    return dice.mean().item()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training utilities\ndef create_optimizer(model: nn.Module, cfg: Config) -> Tuple[torch.optim.Optimizer, CosineAnnealingLR]:\n    \"\"\"Build AdamW optimizer and cosine scheduler for the parameters left trainable.\"\"\"\n    params = [p for p in model.parameters() if p.requires_grad]\n    optimizer = torch.optim.AdamW(params, lr=cfg.base_learning_rate, weight_decay=cfg.weight_decay)\n    scheduler = CosineAnnealingLR(\n        optimizer, T_max=cfg.scheduler_period, eta_min=cfg.min_learning_rate\n    )\n    return optimizer, scheduler\n\n\ndef _next_batch(\n    iterator: Iterator[Dict[str, torch.Tensor]], loader: DataLoader\n) -> Tuple[Dict[str, torch.Tensor], Iterator[Dict[str, torch.Tensor]]]:\n    \"\"\"Fetch the next batch and restart the iterator when the dataloader is exhausted.\"\"\"\n    try:\n        batch = next(iterator)\n    except StopIteration:\n        iterator = iter(loader)\n        batch = next(iterator)\n    return batch, iterator\n\n\ndef train_one_epoch(\n    model: nn.Module,\n    classification_loader: DataLoader,\n    segmentation_loader: DataLoader,\n    classification_loss_fn: nn.Module,\n    segmentation_loss_fn: nn.Module,\n    optimizer: torch.optim.Optimizer,\n    cfg: Config,\n    device: torch.device,\n    scaler: Optional[GradScaler] = None,\n ) -> Dict[str, float]:\n    \"\"\"Train the network for a single epoch while logging both task metrics.\"\"\"\n    model.train()\n    use_amp = scaler is not None and scaler.is_enabled()\n    classification_iter = iter(classification_loader)\n    segmentation_iter = iter(segmentation_loader)\n    max_steps = max(len(classification_loader), len(segmentation_loader))\n    total_loss = 0.0\n    total_cls_loss = 0.0\n    total_seg_loss = 0.0\n    total_accuracy = 0.0\n    total_dice = 0.0\n    for step in range(max_steps):\n        optimizer.zero_grad(set_to_none=True)\n        classification_batch, classification_iter = _next_batch(classification_iter, classification_loader)\n        segmentation_batch, segmentation_iter = _next_batch(segmentation_iter, segmentation_loader)\n        classification_images = classification_batch[\"image\"].to(device, non_blocking=True)\n        classification_labels = classification_batch[\"label\"].to(device, non_blocking=True)\n        segmentation_images = segmentation_batch[\"image\"].to(device, non_blocking=True)\n        segmentation_masks = segmentation_batch[\"mask\"].to(device, non_blocking=True).long()\n        amp_context = autocast() if use_amp and device.type == \"cuda\" else nullcontext()\n        with amp_context:\n            # Classification branch: cross-entropy plus α-weighted fusion regularizer\n            classification_result = model(classification_images, task=\"classification\")\n            cls_logits = classification_result[\"classification\"]\n            fusion_reg = classification_result.get(\"fusion_reg\")\n            if fusion_reg is None:\n                fusion_reg = cls_logits.new_zeros(())\n            cls_loss = classification_loss_fn(cls_logits, classification_labels) + cfg.alpha * fusion_reg\n            # Segmentation branch: pixel-wise cross-entropy between logits and integer mask\n            segmentation_result = model(segmentation_images, task=\"segmentation\")\n            seg_logits = segmentation_result[\"segmentation\"]\n            seg_loss = segmentation_loss_fn(seg_logits, segmentation_masks)\n            loss = cfg.classification_loss_weight * cls_loss + cfg.segmentation_loss_weight * seg_loss\n        if use_amp and device.type == \"cuda\":\n            scaler.scale(loss).backward()\n            if cfg.grad_clip_norm and cfg.grad_clip_norm > 0:\n                scaler.unscale_(optimizer)\n                nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip_norm)\n            scaler.step(optimizer)\n            scaler.update()\n        else:\n            loss.backward()\n            if cfg.grad_clip_norm and cfg.grad_clip_norm > 0:\n                nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip_norm)\n            optimizer.step()\n        total_loss += loss.item()\n        total_cls_loss += cls_loss.item()\n        total_seg_loss += seg_loss.item()\n        total_accuracy += classification_accuracy(cls_logits.detach(), classification_labels)\n        total_dice += dice_score(seg_logits.detach(), segmentation_masks)\n    steps = float(max_steps)\n    return {\n        \"loss\": total_loss / steps,\n        \"classification_loss\": total_cls_loss / steps,\n        \"segmentation_loss\": total_seg_loss / steps,\n        \"classification_accuracy\": total_accuracy / steps,\n        \"segmentation_dice\": total_dice / steps,\n    }\n\n\n@torch.no_grad()\ndef evaluate(\n    model: nn.Module,\n    classification_loader: DataLoader,\n    segmentation_loader: DataLoader,\n    classification_loss_fn: nn.Module,\n    segmentation_loss_fn: nn.Module,\n    cfg: Config,\n    device: torch.device,\n ) -> Dict[str, float]:\n    \"\"\"Run validation/testing without gradient tracking.\"\"\"\n    model.eval()\n    cls_loss_total = 0.0\n    cls_acc_total = 0.0\n    cls_steps = 0\n    for batch in classification_loader:\n        images = batch[\"image\"].to(device, non_blocking=True)\n        labels = batch[\"label\"].to(device, non_blocking=True)\n        classification_result = model(images, task=\"classification\")\n        cls_logits = classification_result[\"classification\"]\n        fusion_reg = classification_result.get(\"fusion_reg\")\n        if fusion_reg is None:\n            fusion_reg = cls_logits.new_zeros(())\n        cls_loss = classification_loss_fn(cls_logits, labels) + cfg.alpha * fusion_reg\n        cls_loss_total += cls_loss.item()\n        cls_acc_total += classification_accuracy(cls_logits, labels)\n        cls_steps += 1\n    seg_loss_total = 0.0\n    seg_dice_total = 0.0\n    seg_steps = 0\n    for batch in segmentation_loader:\n        images = batch[\"image\"].to(device, non_blocking=True)\n        masks = batch[\"mask\"].to(device, non_blocking=True).long()\n        segmentation_result = model(images, task=\"segmentation\")\n        seg_logits = segmentation_result[\"segmentation\"]\n        seg_loss_total += segmentation_loss_fn(seg_logits, masks).item()\n        seg_dice_total += dice_score(seg_logits, masks)\n        seg_steps += 1\n    cls_steps = max(cls_steps, 1)\n    seg_steps = max(seg_steps, 1)\n    return {\n        \"classification_loss\": cls_loss_total / cls_steps,\n        \"classification_accuracy\": cls_acc_total / cls_steps,\n        \"segmentation_loss\": seg_loss_total / seg_steps,\n        \"segmentation_dice\": seg_dice_total / seg_steps,\n    }","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# High-level training loop\ndef fit(\n    cfg: Config,\n    device: torch.device,\n    output_dir: Optional[Path] = None,\n    resume_from: Optional[Path] = None,\n    save_checkpoints: bool = True,\n) -> Dict[str, List[Dict[str, float]]]:\n    classification_datasets, segmentation_datasets = create_datasets(cfg)\n    classification_loaders, segmentation_loaders = create_dataloaders(\n        cfg, classification_datasets, segmentation_datasets\n    )\n    model = MultiTask(num_classes=len(cfg.class_names)).to(device)\n    classification_loss_fn, segmentation_loss_fn = build_loss_functions(cfg, device)\n    optimizer, scheduler = create_optimizer(model, cfg)\n    use_amp = cfg.mixed_precision and device.type == \"cuda\"\n    scaler = GradScaler(enabled=use_amp) if use_amp else None\n    start_epoch = 0\n    best_val_dice = 0.0\n    history: Dict[str, List[Dict[str, float]]] = {\"train\": [], \"val\": []}\n    if output_dir:\n        output_dir.mkdir(parents=True, exist_ok=True)\n    if resume_from:\n        checkpoint = torch.load(resume_from, map_location=device)\n        model.load_state_dict(checkpoint[\"model\"])\n        optimizer.load_state_dict(checkpoint[\"optimizer\"])\n        scheduler.load_state_dict(checkpoint[\"scheduler\"])\n        if scaler and \"scaler\" in checkpoint:\n            scaler.load_state_dict(checkpoint[\"scaler\"])\n        start_epoch = checkpoint.get(\"epoch\", 0) + 1\n        best_val_dice = checkpoint.get(\"best_val_dice\", best_val_dice)\n        print(f\"Resumed training from {resume_from} at epoch {start_epoch}\")\n    for epoch in range(start_epoch, cfg.max_epochs):\n        train_metrics = train_one_epoch(\n            model,\n            classification_loaders[\"train\"],\n            segmentation_loaders[\"train\"],\n            classification_loss_fn,\n            segmentation_loss_fn,\n            optimizer,\n            cfg,\n            device,\n            scaler,\n        )\n        val_metrics = evaluate(\n            model,\n            classification_loaders[\"val\"],\n            segmentation_loaders[\"val\"],\n            classification_loss_fn,\n            segmentation_loss_fn,\n            cfg,\n            device,\n        )\n        scheduler.step()\n        history[\"train\"].append(train_metrics)\n        history[\"val\"].append(val_metrics)\n        print(\n            f\"Epoch {epoch + 1}/{cfg.max_epochs} | \"\n            f\"Train Loss: {train_metrics['loss']:.4f} | \"\n            f\"Val Acc: {val_metrics['classification_accuracy']:.4f} | \"\n            f\"Val Dice: {val_metrics['segmentation_dice']:.4f}\"\n        )\n        current_val_dice = val_metrics[\"segmentation_dice\"]\n        if save_checkpoints and output_dir:\n            checkpoint = {\n                \"epoch\": epoch,\n                \"model\": model.state_dict(),\n                \"optimizer\": optimizer.state_dict(),\n                \"scheduler\": scheduler.state_dict(),\n                \"cfg\": cfg.__dict__,\n                \"train_metrics\": train_metrics,\n                \"val_metrics\": val_metrics,\n                \"best_val_dice\": max(best_val_dice, current_val_dice),\n            }\n            if scaler:\n                checkpoint[\"scaler\"] = scaler.state_dict()\n            torch.save(checkpoint, output_dir / f\"multitask_resnet50_epoch_{epoch + 1:03d}.pth\")\n        if current_val_dice > best_val_dice:\n            best_val_dice = current_val_dice\n    model.eval()\n    return {\n        \"history\": history,\n        \"model\": model,\n        \"classification_loaders\": classification_loaders,\n        \"segmentation_loaders\": segmentation_loaders,\n        \"best_val_dice\": best_val_dice,\n    }","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results = fit(cfg,device,output_dir=Path(\"artifacts/multitask\"),resume_from=None,save_checkpoints=True,)\nbest_model = results[\"model\"]\nhistory = results[\"history\"]","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchinfo import summary\n\nmodel = MultiTask(num_classes=len(cfg.class_names))\n#summary(model, input_size=(1, 3, *cfg.image_size), col_names=(\"input_size\", \"output_size\", \"num_params\", \"trainable\"))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluate the best_model captured during training on the held-out test splits\nif \"best_model\" not in globals():\n    raise RuntimeError(\"best_model is undefined. Run the training cell that returns best_model before inference.\")\nmodel = best_model.to(device).eval()\nclassification_datasets, segmentation_datasets = create_datasets(cfg)\nclassification_loaders, segmentation_loaders = create_dataloaders(\n    cfg, classification_datasets, segmentation_datasets\n)\nclassification_loss_fn, segmentation_loss_fn = build_loss_functions(cfg, device)\ntest_metrics = evaluate(\n    model,\n    classification_loaders[\"test\"],\n    segmentation_loaders[\"test\"],\n    classification_loss_fn,\n    segmentation_loss_fn,\n    cfg,\n    device,\n)\nprint(\"Test metrics:\")\nfor key, value in test_metrics.items():\n    print(f\"  {key}: {value:.4f}\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the best-trained model checkpoint\nif \"best_model\" not in globals():\n    raise RuntimeError(\"best_model is undefined. Run the training cell that returns best_model before saving.\")\nmodel_to_save = best_model.to(\"cpu\").eval()\nsave_path = Path(\"multitask_resnet50_best.pth\")\nsave_path.parent.mkdir(parents=True, exist_ok=True)\ntorch.save(model_to_save.state_dict(), save_path)\nprint(f\"Saved best model weights to {save_path}\")\nbest_model = model_to_save.to(device)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualise model predictions on a random test sample\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport random\n\nif \"best_model\" not in globals():\n    raise RuntimeError(\"Run training to populate best_model before visualisation.\")\nmodel = best_model.to(device).eval()\n\n# Ensure test datasets/loaders are available\nif \"classification_datasets\" not in globals() or \"segmentation_datasets\" not in globals():\n    classification_datasets, segmentation_datasets = create_datasets(cfg)\nif \"classification_loaders\" not in globals() or \"segmentation_loaders\" not in globals():\n    classification_loaders, segmentation_loaders = create_dataloaders(\n        cfg, classification_datasets, segmentation_datasets\n    )\n\ntest_segmentation_dataset = segmentation_datasets[\"test\"]\nsample_idx = random.randrange(len(test_segmentation_dataset))\nsample = test_segmentation_dataset[sample_idx]\nimage = sample[\"image\"].unsqueeze(0).to(device)\nmask_gt = sample[\"mask\"].numpy()\nimage_id = sample[\"image_id\"]\n\nwith torch.no_grad():\n    outputs = model(image)\n    cls_logits = outputs[\"classification\"]\n    seg_logits = outputs[\"segmentation\"]\n    cls_probs = torch.softmax(cls_logits, dim=1)[0].cpu().numpy()\n    pred_class_idx = int(cls_probs.argmax())\n    pred_class_conf = float(cls_probs[pred_class_idx])\n    pred_class_label = cfg.class_names[pred_class_idx]\n    seg_probs = torch.softmax(seg_logits, dim=1)[0, 1].cpu().numpy()\n\n# Try to recover the ground-truth classification label (if the image exists in the CSV)\nground_truth_label = \"N/A\"\ntest_cls_dataset = classification_datasets[\"test\"]\nfilename_candidate = f\"{image_id}.jpg\"\ntry:\n    cls_index = test_cls_dataset.image_paths.index(filename_candidate)\n    label_vector = test_cls_dataset.label_vectors[cls_index]\n    gt_idx = int(label_vector.argmax())\n    ground_truth_label = cfg.class_names[gt_idx]\nexcept ValueError:\n    pass\n\n# Denormalise image tensor for visualisation\nmean = np.array(cfg.imagenet_mean)\nstd = np.array(cfg.imagenet_std)\nimage_np = sample[\"image\"].permute(1, 2, 0).cpu().numpy()\nimage_np = (image_np * std) + mean\nimage_np = np.clip(image_np, 0.0, 1.0)\n\nfig, axes = plt.subplots(1, 3, figsize=(14, 4))\naxes[0].imshow(image_np)\naxes[0].set_title(f\"Input image: {image_id}\")\naxes[0].axis(\"off\")\n\naxes[1].imshow(mask_gt, cmap=\"gray\")\naxes[1].set_title(\"Ground-truth mask\")\naxes[1].axis(\"off\")\n\naxes[2].imshow(image_np)\naxes[2].imshow(seg_probs, cmap=\"viridis\", alpha=0.5)\naxes[2].set_title(\"Predicted mask overlay\")\naxes[2].axis(\"off\")\n\nplt.suptitle(\n    f\"Predicted label: {pred_class_label} (p={pred_class_conf:.2f}) | Ground truth: {ground_truth_label}\"\n)\nplt.tight_layout()\nplt.show()","metadata":{},"outputs":[],"execution_count":null}]}