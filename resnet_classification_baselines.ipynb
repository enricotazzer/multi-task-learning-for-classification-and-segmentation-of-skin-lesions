{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64b7ab3e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# ResNet-50 Baseline\n",
    "Use this notebook to fine-tune a ResNet-50 classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f89cd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from torchvision.models import ResNet50_Weights\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce58b102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 7 classes: ['MEL', 'NV', 'BCC', 'AKIEC', 'BKL', 'DF', 'VASC']\n",
      "Example: ISIC_0024306 -> [0. 1. 0. 0. 0. 0. 0.]\n",
      "Training on mps\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "data_root = Path(\"/Users/enricotazzer/Desktop/multi-task-learning-for-classification-and-segmentation-of-skin-lesions/dataset/classification\")\n",
    "train_dir = data_root / \"train\"\n",
    "val_dir = data_root / \"val\"\n",
    "test_dir = data_root / \"test\"\n",
    "\n",
    "train_img_dir = train_dir / \"input\"\n",
    "val_img_dir = val_dir / \"input\"\n",
    "test_img_dir = test_dir / \"input\"\n",
    "\n",
    "required_paths = [train_img_dir, val_img_dir]\n",
    "for path in required_paths:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Missing required directory: {path}\")\n",
    "\n",
    "train_csv = train_dir / \"ground_truth\" / \"ISIC2018_Task3_Training_GroundTruth.csv\"\n",
    "val_csv = val_dir / \"ground_truth\" / \"ISIC2018_Task3_Validation_GroundTruth.csv\"\n",
    "test_csv = test_dir / \"ground_truth\" / \"ISIC2018_Task3_Test_GroundTruth.csv\"\n",
    "\n",
    "for csv_path in [train_csv, val_csv]:\n",
    "    if not csv_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing ground-truth CSV: {csv_path}\")\n",
    "\n",
    "df_train = pd.read_csv(train_csv)\n",
    "label_columns = [col for col in df_train.columns if col != \"image\"]\n",
    "\n",
    "def prepare_split(df: pd.DataFrame, split_name: str) -> pd.DataFrame:\n",
    "    missing = set(label_columns) - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Columns {missing} are missing from the {split_name} annotations\")\n",
    "    ordered = df[['image'] + label_columns].copy()\n",
    "    return ordered\n",
    "\n",
    "df_val = prepare_split(pd.read_csv(val_csv), \"validation\")\n",
    "df_test = prepare_split(pd.read_csv(test_csv), \"test\") if test_csv.exists() else None\n",
    "\n",
    "class_names = label_columns\n",
    "num_classes = len(class_names)\n",
    "\n",
    "example_row = df_train.iloc[0]\n",
    "example_vector = example_row[label_columns].to_numpy(dtype=np.float32)\n",
    "print(f\"Detected {num_classes} classes: {class_names}\")\n",
    "print(f\"Example: {example_row['image']} -> {example_vector}\")\n",
    "\n",
    "batch_size = 32\n",
    "num_epochs = 25\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-4\n",
    "label_smoothing = 0.1\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Training on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a430b535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 10015 | Val samples: 193\n",
      "Test samples: 1512\n"
     ]
    }
   ],
   "source": [
    "# Data pipeline\n",
    "weights = ResNet50_Weights.DEFAULT\n",
    "\n",
    "\n",
    "def resolve_normalization_params(weights_enum):\n",
    "    \"\"\"Extract mean/std from torchvision weights across API variants.\"\"\"\n",
    "    transforms_obj = weights_enum.transforms()\n",
    "    direct_mean = getattr(transforms_obj, \"mean\", None)\n",
    "    direct_std = getattr(transforms_obj, \"std\", None)\n",
    "    if direct_mean is not None and direct_std is not None:\n",
    "        return tuple(float(m) for m in direct_mean), tuple(float(s) for s in direct_std)\n",
    "\n",
    "    for transform in getattr(transforms_obj, \"transforms\", []):\n",
    "        nested_mean = getattr(transform, \"mean\", None)\n",
    "        nested_std = getattr(transform, \"std\", None)\n",
    "        if nested_mean is not None and nested_std is not None:\n",
    "            return tuple(float(m) for m in nested_mean), tuple(float(s) for s in nested_std)\n",
    "\n",
    "    meta = getattr(weights_enum, \"meta\", None)\n",
    "    if isinstance(meta, dict):\n",
    "        meta_mean = meta.get(\"mean\")\n",
    "        meta_std = meta.get(\"std\")\n",
    "        if meta_mean is not None and meta_std is not None:\n",
    "            return tuple(float(m) for m in meta_mean), tuple(float(s) for s in meta_std)\n",
    "\n",
    "    print(\"Warning: falling back to ImageNet mean/std defaults.\")\n",
    "    return (0.485, 0.456, 0.406), (0.229, 0.224, 0.225)\n",
    "\n",
    "\n",
    "mean, std = resolve_normalization_params(weights)\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std),\n",
    "])\n",
    "\n",
    "eval_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std),\n",
    "])\n",
    "\n",
    "class LesionDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_dir: Path,\n",
    "        annotations: pd.DataFrame,\n",
    "        transform=None,\n",
    "        image_ext: str = \".jpg\",\n",
    "    ):\n",
    "        self.image_dir = Path(image_dir)\n",
    "        if not self.image_dir.exists():\n",
    "            raise FileNotFoundError(f\"Image directory does not exist: {self.image_dir}\")\n",
    "        self.transform = transform\n",
    "        self.image_ext = image_ext\n",
    "        self.label_columns = [col for col in annotations.columns if col != \"image\"]\n",
    "        self.image_ids = annotations[\"image\"].tolist()\n",
    "        label_array = annotations[self.label_columns].to_numpy(dtype=np.float32)\n",
    "        self.targets = torch.as_tensor(label_array, dtype=torch.float32)\n",
    "        self.samples = []\n",
    "        missing_files = []\n",
    "        for image_id in self.image_ids:\n",
    "            path = self._resolve_image_path(image_id)\n",
    "            if path.exists():\n",
    "                self.samples.append(path)\n",
    "            else:\n",
    "                missing_files.append(path.name)\n",
    "        if missing_files:\n",
    "            raise FileNotFoundError(\n",
    "                f\"{len(missing_files)} images listed in annotations were not found in {self.image_dir}. \"\n",
    "                f\"First few missing files: {missing_files[:5]}\"\n",
    "            )\n",
    "        if len(self.samples) != len(self.targets):\n",
    "            raise RuntimeError(\"Mismatch between images and targets after validation.\")\n",
    "\n",
    "    def _resolve_image_path(self, image_id: str) -> Path:\n",
    "        image_name = image_id if Path(image_id).suffix else f\"{image_id}{self.image_ext}\"\n",
    "        return self.image_dir / image_name\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        image_path = self.samples[index]\n",
    "        target = self.targets[index]\n",
    "        with Image.open(image_path) as img:\n",
    "            image = img.convert(\"RGB\")\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return image, target\n",
    "\n",
    "train_dataset = LesionDataset(train_img_dir, df_train, transform=train_transforms)\n",
    "val_dataset = LesionDataset(val_img_dir, df_val, transform=eval_transforms)\n",
    "test_dataset = (\n",
    "    LesionDataset(test_img_dir, df_test, transform=eval_transforms)\n",
    "    if df_test is not None and test_img_dir.exists()\n",
    "    else None\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = (\n",
    "    DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    if test_dataset is not None\n",
    "    else None\n",
    ")\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)} | Val samples: {len(val_dataset)}\")\n",
    "if test_dataset is not None:\n",
    "    print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d38bc21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 14,979,079\n"
     ]
    }
   ],
   "source": [
    "# Model setup\n",
    "model = models.resnet50(weights=weights)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.layer4.parameters():\n",
    "    param.requires_grad = True\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Dropout(p=0.2),\n",
    "    nn.Linear(model.fc.in_features, num_classes)\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
    "optimizer = torch.optim.AdamW((p for p in model.parameters() if p.requires_grad), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=learning_rate * 0.1)\n",
    "\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "887f8212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training utilities\n",
    "def run_epoch(model, dataloader, criterion, optimizer=None):\n",
    "    is_train = optimizer is not None\n",
    "    model.train(is_train)\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    total_samples = 0\n",
    "    start = time.time()\n",
    "\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs = inputs.to(device, non_blocking=True)\n",
    "        targets = targets.to(device, non_blocking=True)\n",
    "        target_indices = torch.argmax(targets, dim=1)\n",
    "\n",
    "        if is_train:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        with torch.set_grad_enabled(is_train):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, target_indices)\n",
    "            if is_train:\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "                optimizer.step()\n",
    "\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        batch_size = inputs.size(0)\n",
    "        running_loss += loss.item() * batch_size\n",
    "        running_corrects += torch.sum(preds == target_indices).item()\n",
    "        total_samples += batch_size\n",
    "\n",
    "    epoch_loss = running_loss / max(total_samples, 1)\n",
    "    epoch_acc = running_corrects / max(total_samples, 1)\n",
    "    elapsed = time.time() - start\n",
    "    return epoch_loss, epoch_acc, elapsed\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs = inputs.to(device, non_blocking=True)\n",
    "        targets = targets.to(device, non_blocking=True)\n",
    "        target_indices = torch.argmax(targets, dim=1)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, target_indices)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        batch_size = inputs.size(0)\n",
    "        running_loss += loss.item() * batch_size\n",
    "        running_corrects += torch.sum(preds == target_indices).item()\n",
    "        total_samples += batch_size\n",
    "\n",
    "        all_preds.append(preds.detach().cpu())\n",
    "        all_targets.append(target_indices.detach().cpu())\n",
    "\n",
    "    epoch_loss = running_loss / max(total_samples, 1)\n",
    "    epoch_acc = running_corrects / max(total_samples, 1)\n",
    "\n",
    "    if all_preds:\n",
    "        all_preds = torch.cat(all_preds).numpy()\n",
    "        all_targets = torch.cat(all_targets).numpy()\n",
    "    else:\n",
    "        all_preds = np.array([])\n",
    "        all_targets = np.array([])\n",
    "\n",
    "    return epoch_loss, epoch_acc, all_preds, all_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "395f2ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler=None, num_epochs=10, checkpoint_dir=\"checkpoints\"):\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "\n",
    "    checkpoint_dir = Path(checkpoint_dir)\n",
    "    checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "    best_ckpt_path = checkpoint_dir / \"resnet50_best.pt\"\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "        train_loss, train_acc, train_time = run_epoch(model, train_loader, criterion, optimizer)\n",
    "        val_loss, val_acc, val_preds, val_targets = evaluate(model, val_loader, criterion)\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "        print(f\"train loss: {train_loss:.4f} | train acc: {train_acc:.4f} | time: {train_time:.1f}s\")\n",
    "        print(f\"val   loss: {val_loss:.4f} | val   acc: {val_acc:.4f}\")\n",
    "\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            torch.save({\n",
    "                \"model_state_dict\": best_model_wts,\n",
    "                \"val_acc\": best_acc,\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"class_names\": class_names,\n",
    "            }, best_ckpt_path)\n",
    "            print(f\"\\nâœ… Saved new best checkpoint to {best_ckpt_path}\\n\")\n",
    "\n",
    "    print(f\"Best val acc: {best_acc:.4f}\")\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85845829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"<string>\"\u001b[0m, line \u001b[35m1\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    from multiprocessing.spawn import spawn_main; \u001b[31mspawn_main\u001b[0m\u001b[1;31m(tracker_fd=79, pipe_handle=94)\u001b[0m\n",
      "                                                  \u001b[31m~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/spawn.py\"\u001b[0m, line \u001b[35m122\u001b[0m, in \u001b[35mspawn_main\u001b[0m\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \u001b[35m\"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/spawn.py\"\u001b[0m, line \u001b[35m132\u001b[0m, in \u001b[35m_main\u001b[0m\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "\u001b[1;35mAttributeError\u001b[0m: \u001b[35mCan't get attribute 'LesionDataset' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:1\u001b[0m\n",
      "Cell \u001b[0;32mIn[6], line 14\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, checkpoint_dir)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m20\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m train_loss, train_acc, train_time \u001b[38;5;241m=\u001b[39m \u001b[43mrun_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m val_loss, val_acc, val_preds, val_targets \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader, criterion)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m scheduler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[5], line 10\u001b[0m, in \u001b[0;36mrun_epoch\u001b[0;34m(model, dataloader, criterion, optimizer)\u001b[0m\n\u001b[1;32m      7\u001b[0m total_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      8\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 10\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/utils/data/dataloader.py:493\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    491\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 493\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/utils/data/dataloader.py:424\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[0;32m--> 424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/utils/data/dataloader.py:1171\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1164\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[1;32m   1167\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[1;32m   1168\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[1;32m   1169\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[1;32m   1170\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[0;32m-> 1171\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1172\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/context.py:289\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_posix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, process_obj):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/popen_fork.py:20\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinalizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/popen_spawn_posix.py:62\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentinel \u001b[38;5;241m=\u001b[39m parent_r\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(parent_w, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m, closefd\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 62\u001b[0m         \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetbuffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     fds_to_close \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "if __name__ == \"__main__\":\n",
    "    if len(train_dataset) == 0 or len(val_dataset) == 0:\n",
    "        raise RuntimeError(\"Training/validation datasets are empty. Check the data directory structure.\")\n",
    "\n",
    "    %time trained_model, history = train_model(model=model, train_loader=train_loader, val_loader=val_loader, criterion=criterion, optimizer=optimizer, scheduler=scheduler, num_epochs=num_epochs, checkpoint_dir=\"artifacts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9410f467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning curves\n",
    "if 'history' in locals():\n",
    "    import matplotlib.pyplot as plt\n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    axes[0].plot(epochs, history['train_loss'], label='Train')\n",
    "    axes[0].plot(epochs, history['val_loss'], label='Val')\n",
    "    axes[0].set_title('Loss')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Cross-Entropy')\n",
    "    axes[0].legend()\n",
    "\n",
    "    axes[1].plot(epochs, history['train_acc'], label='Train')\n",
    "    axes[1].plot(epochs, history['val_acc'], label='Val')\n",
    "    axes[1].set_title('Accuracy')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Accuracy')\n",
    "    axes[1].legend()\n",
    "    plt.tight_layout()\n",
    "else:\n",
    "    print(\"Run the training cell first to generate history.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b787465b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation metrics\n",
    "if 'trained_model' in locals():\n",
    "    val_loss, val_acc, val_preds, val_targets = evaluate(trained_model, val_loader, criterion)\n",
    "    print(f\"Validation loss: {val_loss:.4f}\")\n",
    "    print(f\"Validation acc : {val_acc:.4f}\")\n",
    "    if val_targets.size > 0:\n",
    "        print(classification_report(val_targets, val_preds, target_names=class_names, digits=4))\n",
    "        try:\n",
    "            import matplotlib.pyplot as plt\n",
    "            import seaborn as sns\n",
    "        except ImportError:\n",
    "            print(\"Install seaborn and matplotlib to visualize the confusion matrix.\")\n",
    "        else:\n",
    "            cm = confusion_matrix(val_targets, val_preds)\n",
    "            fig, ax = plt.subplots(figsize=(6, 6))\n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names, ax=ax)\n",
    "            ax.set_xlabel('Predicted')\n",
    "            ax.set_ylabel('True')\n",
    "            ax.set_title('Validation Confusion Matrix')\n",
    "            plt.tight_layout()\n",
    "    else:\n",
    "        print(\"No validation predictions captured. This can happen if the dataset is empty.\")\n",
    "else:\n",
    "    print(\"Train the model before running this cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d727644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set evaluation\n",
    "if test_loader is not None and 'trained_model' in locals():\n",
    "    test_loss, test_acc, test_preds, test_targets = evaluate(trained_model, test_loader, criterion)\n",
    "    print(f\"Test loss: {test_loss:.4f}\")\n",
    "    print(f\"Test acc : {test_acc:.4f}\")\n",
    "else:\n",
    "    print(\"No test set detected or model has not been trained yet.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
