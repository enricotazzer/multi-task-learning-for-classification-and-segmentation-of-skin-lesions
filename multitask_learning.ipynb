{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ff56fe5",
   "metadata": {},
   "source": [
    "# Multi-Task Skin Lesion Diagnostic\n",
    "This notebook sketches a PyTorch implementation of the architecture described in *Multi-Task Classification and Segmentation for Explicable Capsule Endoscopy Diagnostics*. The model shares an encoder across tasks and uses separate heads for frame-level classification and pixel-level lesion segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0021a4ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1155eb3d0>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, Iterator, List, Optional, Tuple\n",
    "\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.models import ResNet50_Weights, resnet50\n",
    "from torchvision.transforms import InterpolationMode\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "torch.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e395b4f2",
   "metadata": {},
   "source": [
    "# Model configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "14b86aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "@dataclass\n",
    "class Config:\n",
    "    project_root: Path = Path.cwd()\n",
    "    image_size: Tuple[int, int] = (256, 256)\n",
    "    batch_size: int = 8\n",
    "    segmentation_batch_size: int = 4\n",
    "    base_learning_rate: float = 3e-4\n",
    "    weight_decay: float = 1e-4\n",
    "    max_epochs: int = 40\n",
    "    classification_loss_weight: float = 0.4\n",
    "    segmentation_loss_weight: float = 1.0\n",
    "    class_names: Tuple[str, ...] = (\"MEL\", \"NV\", \"BCC\", \"AKIEC\", \"BKL\", \"DF\", \"VASC\")\n",
    "    ignore_index: int = 255\n",
    "    min_learning_rate: float = 1e-6\n",
    "    scheduler_period: int = 10\n",
    "    mixed_precision: bool = True\n",
    "    segmentation_positive_weight: float = 1.5\n",
    "    grad_clip_norm: Optional[float] = 5.0\n",
    "    imagenet_mean: Tuple[float, float, float] = (0.485, 0.456, 0.406)\n",
    "    imagenet_std: Tuple[float, float, float] = (0.229, 0.224, 0.225)\n",
    "    segmentation_suffix: str = \"_segmentation\"\n",
    "    csv_path = '/Users/enricotazzer/Desktop/multi-task-learning-for-classification-and-segmentation-of-skin-lesions/mutlitask_dataset/segmentation_labels.csv'\n",
    "\n",
    "    def segmentation_input_dir(self, split: str) -> Path:\n",
    "        return self.project_root / \"dataset\" / \"segmentation\" / split / \"input\"\n",
    "\n",
    "    def segmentation_mask_dir(self, split: str) -> Path:\n",
    "        return self.project_root / \"dataset\" / \"segmentation\" / split / \"ground_truth\"\n",
    "    \n",
    "    def classification_labels(self, split: str) -> Path:\n",
    "        df = pd.read_csv(self.csv_path)\n",
    "        mask = df[\"image\"].astype(str).str.contains(split, na=False)\n",
    "        return df.loc[mask].reset_index(drop=True)\n",
    "    \n",
    "cfg = Config()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3538c31",
   "metadata": {},
   "source": [
    "# Image utilites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c7e5c664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transforms\n",
    "def build_classification_transform(cfg: Config, train: bool) -> transforms.Compose:\n",
    "    augmentations: List[transforms.Compose]\n",
    "    if train:\n",
    "        augmentations = [\n",
    "            transforms.RandomResizedCrop(cfg.image_size, scale=(0.8, 1.0), interpolation=InterpolationMode.BILINEAR),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomVerticalFlip(p=0.1),\n",
    "            transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n",
    "        ]\n",
    "    else:\n",
    "        augmentations = [\n",
    "            transforms.Resize(cfg.image_size, interpolation=InterpolationMode.BILINEAR),\n",
    "            transforms.CenterCrop(cfg.image_size),\n",
    "        ]\n",
    "    augmentations += [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=cfg.imagenet_mean, std=cfg.imagenet_std),\n",
    "    ]\n",
    "    return transforms.Compose(augmentations)\n",
    "\n",
    "\n",
    "def apply_segmentation_transforms(\n",
    "    image: Image.Image, mask: Image.Image, cfg: Config, train: bool\n",
    " ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Apply spatial transforms jointly to the image and mask.\"\"\"\n",
    "    image = image.convert(\"RGB\")\n",
    "    mask = mask.convert(\"L\")\n",
    "    if train:\n",
    "        if random.random() < 0.5:\n",
    "            image = TF.hflip(image)\n",
    "            mask = TF.hflip(mask)\n",
    "        if random.random() < 0.2:\n",
    "            image = TF.vflip(image)\n",
    "            mask = TF.vflip(mask)\n",
    "        if random.random() < 0.3:\n",
    "            angle = random.uniform(-15.0, 15.0)\n",
    "            image = TF.rotate(image, angle, interpolation=InterpolationMode.BILINEAR)\n",
    "            mask = TF.rotate(mask, angle, interpolation=InterpolationMode.NEAREST)\n",
    "    image = TF.resize(image, cfg.image_size, interpolation=InterpolationMode.BILINEAR)\n",
    "    mask = TF.resize(mask, cfg.image_size, interpolation=InterpolationMode.NEAREST)\n",
    "    image_tensor = TF.normalize(TF.to_tensor(image), mean=cfg.imagenet_mean, std=cfg.imagenet_std)\n",
    "    mask_tensor = torch.from_numpy(np.array(mask, dtype=np.uint8))\n",
    "    mask_tensor = (mask_tensor > 0).float().unsqueeze(0)\n",
    "    return image_tensor, mask_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa6edad",
   "metadata": {},
   "source": [
    "# Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "162e7981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset definitions\n",
    "class ISICClassificationDataset(Dataset):\n",
    "    def __init__(self, cfg: Config, split: str):\n",
    "        self.cfg = cfg\n",
    "        self.split = split\n",
    "        self.transform = build_classification_transform(self.cfg, self.split == 'train')\n",
    "        self.csv_path = cfg.csv_path\n",
    "        if not Path(self.csv_path).exists():\n",
    "            raise FileNotFoundError(f\"Missing classification CSV at {self.csv_path}\")\n",
    "        self.metadata = cfg.classification_labels(split)\n",
    "        if self.metadata.empty:\n",
    "            raise RuntimeError(f\"Classification CSV at {self.csv_path} is empty\")\n",
    "        self.image_paths = self.metadata[\"image\"].tolist()\n",
    "        self.label_vectors = self.metadata.loc[:, cfg.class_names].values.astype(np.float32)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def _resolve_image_path(self, image_id: str) -> Path:\n",
    "        path = f'mutlitask_dataset/segmentation/{image_id}'\n",
    "        if Path(path).exists():\n",
    "            return path\n",
    "        raise FileNotFoundError(f\"Could not locate an image file for id '{image_id}' in {self.image_paths}\")\n",
    "\n",
    "    def __getitem__(self, index: int) -> Dict[str, torch.Tensor]:\n",
    "        image_id = self.image_paths[index]\n",
    "        image_path = self._resolve_image_path(image_id)\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        tensor = self.transform(image)\n",
    "        label_vector = self.label_vectors[index]\n",
    "        label = torch.tensor(int(label_vector.argmax()), dtype=torch.long)\n",
    "        return {\n",
    "            \"image\": tensor,\n",
    "            \"label\": label,\n",
    "            \"label_one_hot\": torch.from_numpy(label_vector),\n",
    "            \"image_id\": image_id,\n",
    "        }\n",
    "\n",
    "\n",
    "class ISICSegmentationDataset(Dataset):\n",
    "    def __init__(self, cfg: Config, split: str):\n",
    "        self.cfg = cfg\n",
    "        self.split = split\n",
    "        self.images_dir = cfg.segmentation_input_dir(split)\n",
    "        self.masks_dir = cfg.segmentation_mask_dir(split)\n",
    "        self.image_paths = sorted([p for p in self.images_dir.glob(\"*.jpg\")])\n",
    "        if not self.image_paths:\n",
    "            raise RuntimeError(f\"No segmentation images found under {self.images_dir}\")\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def _mask_path(self, image_path: Path) -> Path:\n",
    "        mask_name = f\"{image_path.stem}{self.cfg.segmentation_suffix}.png\"\n",
    "        return self.masks_dir / mask_name\n",
    "\n",
    "    def __getitem__(self, index: int) -> Dict[str, torch.Tensor]:\n",
    "        image_path = self.image_paths[index]\n",
    "        mask_path = self._mask_path(image_path)\n",
    "        if not mask_path.exists():\n",
    "            raise FileNotFoundError(f\"Missing mask for {image_path.name} at {mask_path}\")\n",
    "        image = Image.open(image_path)\n",
    "        mask = Image.open(mask_path)\n",
    "        image_tensor, mask_tensor = apply_segmentation_transforms(\n",
    "            image, mask, self.cfg, train=self.split == \"train\"\n",
    "        )\n",
    "        return {\n",
    "            \"image\": image_tensor,\n",
    "            \"mask\": mask_tensor,\n",
    "            \"image_id\": image_path.stem,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9347fa1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaders created succesfully\n"
     ]
    }
   ],
   "source": [
    "# DataModule-style helpers\n",
    "def create_datasets(cfg: Config) -> Tuple[Dict[str, Dataset], Dict[str, Dataset]]:\n",
    "    classification = {split: ISICClassificationDataset(cfg, split) for split in (\"train\", \"val\", \"test\")}\n",
    "    segmentation = {split: ISICSegmentationDataset(cfg, split) for split in (\"train\", \"val\", \"test\")}\n",
    "    return classification, segmentation\n",
    "\n",
    "\n",
    "def create_dataloaders(\n",
    "    cfg: Config,\n",
    "    classification: Dict[str, Dataset],\n",
    "    segmentation: Dict[str, Dataset],\n",
    ") -> Tuple[Dict[str, DataLoader], Dict[str, DataLoader]]:\n",
    "    classification_loaders = {\n",
    "        split: DataLoader(\n",
    "            dataset,\n",
    "            batch_size=cfg.batch_size,\n",
    "            shuffle=split == \"train\",\n",
    "            drop_last=split == \"train\"\n",
    "        )\n",
    "        for split, dataset in classification.items()\n",
    "    }\n",
    "    segmentation_loaders = {\n",
    "        split: DataLoader(\n",
    "            dataset,\n",
    "            batch_size=cfg.segmentation_batch_size,\n",
    "            shuffle=split == \"train\",\n",
    "            drop_last=split == \"train\"\n",
    "        )\n",
    "        for split, dataset in segmentation.items()\n",
    "    }\n",
    "    return classification_loaders, segmentation_loaders\n",
    "\n",
    "classification, segmentation = create_datasets(cfg)\n",
    "classification_loader, segmentation_loader = create_dataloaders(cfg, classification, segmentation) \n",
    "# loaders are dict of the form {train: train_loader, val: val_loader, test: test_loader}\n",
    "print(\"Loaders created succesfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea17f400",
   "metadata": {},
   "source": [
    "## Dataset sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9a19d611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 2594, 'val': 100, 'test': 1000}\n",
      "{'train': 2594, 'val': 100, 'test': 1000}\n",
      "Classification sample: torch.Size([3, 256, 256]) tensor([0., 1., 0., 0., 0., 0., 0.]) train/input/ISIC_0000000.jpg\n",
      "Segmentation sample: torch.Size([3, 256, 256]) torch.Size([1, 256, 256]) ISIC_0000000\n"
     ]
    }
   ],
   "source": [
    "# Quick dataset sanity check\n",
    "classification_datasets, segmentation_datasets = create_datasets(cfg)\n",
    "print({split: len(ds) for split, ds in classification_datasets.items()})\n",
    "print({split: len(ds) for split, ds in segmentation_datasets.items()})\n",
    "sample_cls = classification_datasets[\"train\"][0]\n",
    "sample_seg = segmentation_datasets[\"train\"][0]\n",
    "print(\"Classification sample:\", sample_cls[\"image\"].shape, sample_cls[\"label_one_hot\"], sample_cls[\"image_id\"])\n",
    "print(\"Segmentation sample:\", sample_seg[\"image\"].shape, sample_seg[\"mask\"].shape, sample_seg[\"image_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c7cc82",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f77b875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model components\n",
    "class ConvBNReLU(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int = 3, padding: int = 1):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=padding, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class SegmentationDecoder(nn.Module):\n",
    "    def __init__(self, channels: Tuple[int, int, int, int, int], num_classes: int):\n",
    "        super().__init__()\n",
    "        c0, c1, c2, c3, c4 = channels\n",
    "        self.block4 = ConvBNReLU(c4, 512, kernel_size=1, padding=0)\n",
    "        self.block3 = ConvBNReLU(512 + c3, 256)\n",
    "        self.block2 = ConvBNReLU(256 + c2, 128)\n",
    "        self.block1 = ConvBNReLU(128 + c1, 96)\n",
    "        self.block0 = ConvBNReLU(96 + c0, 64)\n",
    "        self.head = nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, features: Tuple[torch.Tensor, ...]) -> torch.Tensor:\n",
    "        c0, c1, c2, c3, c4 = features\n",
    "        x = self.block4(c4)\n",
    "        x = F.interpolate(x, size=c3.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "        x = torch.cat([x, c3], dim=1)\n",
    "        x = self.block3(x)\n",
    "        x = F.interpolate(x, size=c2.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "        x = torch.cat([x, c2], dim=1)\n",
    "        x = self.block2(x)\n",
    "        x = F.interpolate(x, size=c1.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "        x = torch.cat([x, c1], dim=1)\n",
    "        x = self.block1(x)\n",
    "        x = F.interpolate(x, size=c0.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "        x = torch.cat([x, c0], dim=1)\n",
    "        x = self.block0(x)\n",
    "        return self.head(x)\n",
    "\n",
    "\n",
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, in_channels: int, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        self.fc = nn.Linear(in_channels, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.pool(x).flatten(1)\n",
    "        x = self.dropout(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "class MultiTaskResNet50(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int,\n",
    "        num_segmentation_classes: int = 1,\n",
    "        trainable_backbone_layers: int = 2,\n",
    "        use_pretrained: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        weights = ResNet50_Weights.DEFAULT if use_pretrained else None\n",
    "        backbone = resnet50(weights=weights)\n",
    "        self.initial = nn.Sequential(backbone.conv1, backbone.bn1, backbone.relu)\n",
    "        self.maxpool = backbone.maxpool\n",
    "        self.layer1 = backbone.layer1\n",
    "        self.layer2 = backbone.layer2\n",
    "        self.layer3 = backbone.layer3\n",
    "        self.layer4 = backbone.layer4\n",
    "        self._set_trainable_layers(trainable_backbone_layers)\n",
    "        self.classifier = ClassificationHead(in_channels=2048, num_classes=num_classes)\n",
    "        self.segmentation_decoder = SegmentationDecoder(\n",
    "            channels=(64, 256, 512, 1024, 2048), num_classes=num_segmentation_classes\n",
    "        )\n",
    "\n",
    "    def _set_trainable_layers(self, trainable_backbone_layers: int) -> None:\n",
    "        if trainable_backbone_layers < 1 or trainable_backbone_layers > 5:\n",
    "            raise ValueError(\"trainable_backbone_layers must be between 1 and 5 for ResNet-50\")\n",
    "        layers = [self.initial, self.layer1, self.layer2, self.layer3, self.layer4]\n",
    "        kept_layers = layers[-trainable_backbone_layers:]\n",
    "        for module in layers:\n",
    "            requires_grad = module in kept_layers\n",
    "            for param in module.parameters():\n",
    "                param.requires_grad = requires_grad\n",
    "\n",
    "    def forward(self, x: torch.Tensor, task: Optional[str] = None) -> Dict[str, torch.Tensor]:\n",
    "        if task and task not in {\"classification\", \"segmentation\"}:\n",
    "            raise ValueError(\"task must be 'classification', 'segmentation', or None\")\n",
    "        input_spatial = x.shape[-2:]\n",
    "        c0 = self.initial(x)\n",
    "        p0 = self.maxpool(c0)\n",
    "        c1 = self.layer1(p0)\n",
    "        c2 = self.layer2(c1)\n",
    "        c3 = self.layer3(c2)\n",
    "        c4 = self.layer4(c3)\n",
    "        features = (p0, c1, c2, c3, c4)\n",
    "        requested = {task} if task else {\"classification\", \"segmentation\"}\n",
    "        outputs: Dict[str, torch.Tensor] = {}\n",
    "        if \"classification\" in requested:\n",
    "            outputs[\"classification\"] = self.classifier(c4)\n",
    "        if \"segmentation\" in requested:\n",
    "            segmentation_logits = self.segmentation_decoder(features)\n",
    "            segmentation_logits = F.interpolate(\n",
    "                segmentation_logits, size=input_spatial, mode=\"bilinear\", align_corners=False\n",
    "            )\n",
    "            outputs[\"segmentation\"] = segmentation_logits\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2387499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Losses and metrics\n",
    "def build_loss_functions(cfg: Config, device: torch.device) -> Tuple[nn.Module, nn.Module]:\n",
    "    classification_loss = nn.CrossEntropyLoss()\n",
    "    pos_weight = torch.tensor([cfg.segmentation_positive_weight], device=device)\n",
    "    segmentation_loss = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    return classification_loss, segmentation_loss\n",
    "\n",
    "\n",
    "def classification_accuracy(logits: torch.Tensor, labels: torch.Tensor) -> float:\n",
    "    predictions = logits.argmax(dim=1)\n",
    "    correct = (predictions == labels).sum().item()\n",
    "    total = labels.numel()\n",
    "    return correct / max(total, 1)\n",
    "\n",
    "\n",
    "def dice_score(logits: torch.Tensor, targets: torch.Tensor, threshold: float = 0.5, eps: float = 1e-6) -> float:\n",
    "    probabilities = torch.sigmoid(logits)\n",
    "    preds = (probabilities > threshold).float()\n",
    "    intersection = (preds * targets).sum(dim=(1, 2, 3))\n",
    "    union = preds.sum(dim=(1, 2, 3)) + targets.sum(dim=(1, 2, 3))\n",
    "    dice = (2 * intersection + eps) / (union + eps)\n",
    "    return dice.mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423063c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training utilities\n",
    "def create_optimizer(model: nn.Module, cfg: Config) -> Tuple[torch.optim.Optimizer, CosineAnnealingLR]:\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.AdamW(params, lr=cfg.base_learning_rate, weight_decay=cfg.weight_decay)\n",
    "    scheduler = CosineAnnealingLR(\n",
    "        optimizer, T_max=cfg.scheduler_period, eta_min=cfg.min_learning_rate\n",
    "    )\n",
    "    return optimizer, scheduler\n",
    "\n",
    "\n",
    "def _next_batch(\n",
    "    iterator: Iterator[Dict[str, torch.Tensor]], loader: DataLoader\n",
    ") -> Tuple[Dict[str, torch.Tensor], Iterator[Dict[str, torch.Tensor]]]:\n",
    "    try:\n",
    "        batch = next(iterator)\n",
    "    except StopIteration:\n",
    "        iterator = iter(loader)\n",
    "        batch = next(iterator)\n",
    "    return batch, iterator\n",
    "\n",
    "\n",
    "def train_one_epoch(\n",
    "    model: nn.Module,\n",
    "    classification_loader: DataLoader,\n",
    "    segmentation_loader: DataLoader,\n",
    "    classification_loss_fn: nn.Module,\n",
    "    segmentation_loss_fn: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    cfg: Config,\n",
    "    device: torch.device,\n",
    "    scaler: Optional[GradScaler] = None,\n",
    ") -> Dict[str, float]:\n",
    "    model.train()\n",
    "    use_amp = scaler is not None and scaler.is_enabled()\n",
    "    classification_iter = iter(classification_loader)\n",
    "    segmentation_iter = iter(segmentation_loader)\n",
    "    max_steps = max(len(classification_loader), len(segmentation_loader))\n",
    "    total_loss = 0.0\n",
    "    total_cls_loss = 0.0\n",
    "    total_seg_loss = 0.0\n",
    "    total_accuracy = 0.0\n",
    "    total_dice = 0.0\n",
    "    for step in range(max_steps):\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        classification_batch, classification_iter = _next_batch(classification_iter, classification_loader)\n",
    "        segmentation_batch, segmentation_iter = _next_batch(segmentation_iter, segmentation_loader)\n",
    "        classification_images = classification_batch[\"image\"].to(device, non_blocking=True)\n",
    "        classification_labels = classification_batch[\"label\"].to(device, non_blocking=True)\n",
    "        segmentation_images = segmentation_batch[\"image\"].to(device, non_blocking=True)\n",
    "        segmentation_masks = segmentation_batch[\"mask\"].to(device, non_blocking=True)\n",
    "        with autocast(device_type=device.type, enabled=use_amp):\n",
    "            classification_outputs = model(classification_images, task=\"classification\")[\"classification\"]\n",
    "            cls_loss = classification_loss_fn(classification_outputs, classification_labels)\n",
    "            segmentation_outputs = model(segmentation_images, task=\"segmentation\")[\"segmentation\"]\n",
    "            seg_loss = segmentation_loss_fn(segmentation_outputs, segmentation_masks)\n",
    "            loss = cfg.classification_loss_weight * cls_loss + cfg.segmentation_loss_weight * seg_loss\n",
    "        if use_amp:\n",
    "            scaler.scale(loss).backward()\n",
    "            if cfg.grad_clip_norm and cfg.grad_clip_norm > 0:\n",
    "                scaler.unscale_(optimizer)\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip_norm)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            if cfg.grad_clip_norm and cfg.grad_clip_norm > 0:\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip_norm)\n",
    "            optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        total_cls_loss += cls_loss.item()\n",
    "        total_seg_loss += seg_loss.item()\n",
    "        total_accuracy += classification_accuracy(classification_outputs.detach(), classification_labels)\n",
    "        total_dice += dice_score(segmentation_outputs.detach(), segmentation_masks)\n",
    "    steps = float(max_steps)\n",
    "    return {\n",
    "        \"loss\": total_loss / steps,\n",
    "        \"classification_loss\": total_cls_loss / steps,\n",
    "        \"segmentation_loss\": total_seg_loss / steps,\n",
    "        \"classification_accuracy\": total_accuracy / steps,\n",
    "        \"segmentation_dice\": total_dice / steps,\n",
    "    }\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(\n",
    "    model: nn.Module,\n",
    "    classification_loader: DataLoader,\n",
    "    segmentation_loader: DataLoader,\n",
    "    classification_loss_fn: nn.Module,\n",
    "    segmentation_loss_fn: nn.Module,\n",
    "    cfg: Config,\n",
    "    device: torch.device,\n",
    ") -> Dict[str, float]:\n",
    "    model.eval()\n",
    "    cls_loss_total = 0.0\n",
    "    cls_acc_total = 0.0\n",
    "    cls_steps = 0\n",
    "    for batch in classification_loader:\n",
    "        images = batch[\"image\"].to(device, non_blocking=True)\n",
    "        labels = batch[\"label\"].to(device, non_blocking=True)\n",
    "        outputs = model(images, task=\"classification\")[\"classification\"]\n",
    "        cls_loss_total += classification_loss_fn(outputs, labels).item()\n",
    "        cls_acc_total += classification_accuracy(outputs, labels)\n",
    "        cls_steps += 1\n",
    "    seg_loss_total = 0.0\n",
    "    seg_dice_total = 0.0\n",
    "    seg_steps = 0\n",
    "    for batch in segmentation_loader:\n",
    "        images = batch[\"image\"].to(device, non_blocking=True)\n",
    "        masks = batch[\"mask\"].to(device, non_blocking=True)\n",
    "        outputs = model(images, task=\"segmentation\")[\"segmentation\"]\n",
    "        seg_loss_total += segmentation_loss_fn(outputs, masks).item()\n",
    "        seg_dice_total += dice_score(outputs, masks)\n",
    "        seg_steps += 1\n",
    "    cls_steps = max(cls_steps, 1)\n",
    "    seg_steps = max(seg_steps, 1)\n",
    "    return {\n",
    "        \"classification_loss\": cls_loss_total / cls_steps,\n",
    "        \"classification_accuracy\": cls_acc_total / cls_steps,\n",
    "        \"segmentation_loss\": seg_loss_total / seg_steps,\n",
    "        \"segmentation_dice\": seg_dice_total / seg_steps,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97cef6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# High-level training loop\n",
    "def fit(\n",
    "    cfg: Config,\n",
    "    device: torch.device,\n",
    "    output_dir: Optional[Path] = None,\n",
    "    resume_from: Optional[Path] = None,\n",
    "    save_checkpoints: bool = True,\n",
    ") -> Dict[str, List[Dict[str, float]]]:\n",
    "    classification_datasets, segmentation_datasets = create_datasets(cfg)\n",
    "    classification_loaders, segmentation_loaders = create_dataloaders(\n",
    "        cfg, classification_datasets, segmentation_datasets\n",
    "    )\n",
    "    model = MultiTaskResNet50(num_classes=len(cfg.class_names)).to(device)\n",
    "    classification_loss_fn, segmentation_loss_fn = build_loss_functions(cfg, device)\n",
    "    optimizer, scheduler = create_optimizer(model, cfg)\n",
    "    use_amp = cfg.mixed_precision and device.type == \"cuda\"\n",
    "    scaler = GradScaler(enabled=use_amp) if use_amp else None\n",
    "    start_epoch = 0\n",
    "    best_val_dice = 0.0\n",
    "    history: Dict[str, List[Dict[str, float]]] = {\"train\": [], \"val\": []}\n",
    "    if output_dir:\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    if resume_from:\n",
    "        checkpoint = torch.load(resume_from, map_location=device)\n",
    "        model.load_state_dict(checkpoint[\"model\"])\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "        scheduler.load_state_dict(checkpoint[\"scheduler\"])\n",
    "        if scaler and \"scaler\" in checkpoint:\n",
    "            scaler.load_state_dict(checkpoint[\"scaler\"])\n",
    "        start_epoch = checkpoint.get(\"epoch\", 0) + 1\n",
    "        best_val_dice = checkpoint.get(\"best_val_dice\", best_val_dice)\n",
    "        print(f\"Resumed training from {resume_from} at epoch {start_epoch}\")\n",
    "    for epoch in range(start_epoch, cfg.max_epochs):\n",
    "        train_metrics = train_one_epoch(\n",
    "            model,\n",
    "            classification_loaders[\"train\"],\n",
    "            segmentation_loaders[\"train\"],\n",
    "            classification_loss_fn,\n",
    "            segmentation_loss_fn,\n",
    "            optimizer,\n",
    "            cfg,\n",
    "            device,\n",
    "            scaler,\n",
    "        )\n",
    "        val_metrics = evaluate(\n",
    "            model,\n",
    "            classification_loaders[\"val\"],\n",
    "            segmentation_loaders[\"val\"],\n",
    "            classification_loss_fn,\n",
    "            segmentation_loss_fn,\n",
    "            cfg,\n",
    "            device,\n",
    "        )\n",
    "        scheduler.step()\n",
    "        history[\"train\"].append(train_metrics)\n",
    "        history[\"val\"].append(val_metrics)\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}/{cfg.max_epochs} | \"\n",
    "            f\"Train Loss: {train_metrics['loss']:.4f} | \"\n",
    "            f\"Val Acc: {val_metrics['classification_accuracy']:.4f} | \"\n",
    "            f\"Val Dice: {val_metrics['segmentation_dice']:.4f}\"\n",
    "        )\n",
    "        current_val_dice = val_metrics[\"segmentation_dice\"]\n",
    "        if save_checkpoints and output_dir:\n",
    "            checkpoint = {\n",
    "                \"epoch\": epoch,\n",
    "                \"model\": model.state_dict(),\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "                \"scheduler\": scheduler.state_dict(),\n",
    "                \"cfg\": cfg.__dict__,\n",
    "                \"train_metrics\": train_metrics,\n",
    "                \"val_metrics\": val_metrics,\n",
    "                \"best_val_dice\": max(best_val_dice, current_val_dice),\n",
    "            }\n",
    "            if scaler:\n",
    "                checkpoint[\"scaler\"] = scaler.state_dict()\n",
    "            torch.save(checkpoint, output_dir / f\"multitask_resnet50_epoch_{epoch + 1:03d}.pth\")\n",
    "        if current_val_dice > best_val_dice:\n",
    "            best_val_dice = current_val_dice\n",
    "    model.eval()\n",
    "    return {\n",
    "        \"history\": history,\n",
    "        \"model\": model,\n",
    "        \"classification_loaders\": classification_loaders,\n",
    "        \"segmentation_loaders\": segmentation_loaders,\n",
    "        \"best_val_dice\": best_val_dice,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6e0aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example training call (disabled by default)\n",
    "# results = fit(\n",
    "#     cfg,\n",
    "#     device,\n",
    "#     output_dir=Path(\"artifacts/multitask\"),\n",
    "#     resume_from=None,\n",
    "#     save_checkpoints=True,\n",
    "# )\n",
    "# best_model = results[\"model\"]\n",
    "# history = results[\"history\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8743dee5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
