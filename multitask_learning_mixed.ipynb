{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ff56fe5",
   "metadata": {},
   "source": [
    "# Multi-Task Skin Lesion Diagnostic\n",
    "This notebook sketches a PyTorch implementation of the architecture described in *Multi-Task Classification and Segmentation for Explicable Capsule Endoscopy Diagnostics*. The model shares an encoder across tasks and uses separate heads for frame-level classification and pixel-level lesion segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0021a4ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11239ebb0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Core libraries, torch modules, and torchvision utilities used throughout the notebook\n",
    "from contextlib import nullcontext\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, Iterator, List, Optional, Tuple\n",
    "\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.models import ResNet50_Weights, resnet50\n",
    "from torchvision.transforms import InterpolationMode\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "# Fix random seed so the data augmentations and weight initialisation remain reproducible\n",
    "torch.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e395b4f2",
   "metadata": {},
   "source": [
    "# Model configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b86aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Central hyper-parameters mirroring the settings reported in the paper.\"\"\"\n",
    "    project_root: Path = Path.cwd()\n",
    "    mixed_dataset_name: str = \"multitasked_dataset_mixed\"\n",
    "    image_size: Tuple[int, int] = (256, 256)  # input resolution for both tasks\n",
    "    batch_size: int = 8  # classification mini-batch size\n",
    "    segmentation_batch_size: int = 4  # segmentation mini-batch size (decoder is heavier)\n",
    "    base_learning_rate: float = 1e-4\n",
    "    weight_decay: float = 1e-4\n",
    "    max_epochs: int = 20\n",
    "    alpha: float = 0.1  # fusion-penalty weight α used in the paper for regularisation\n",
    "    classification_loss_weight: float = 0.7  # balance between classification and segmentation losses\n",
    "    segmentation_loss_weight: float = 0.8\n",
    "    class_names: Tuple[str, ...] = (\"MEL\", \"NV\", \"BCC\", \"AKIEC\", \"BKL\", \"DF\", \"VASC\")\n",
    "    ignore_index: int = 255  # optional label to mask out invalid pixels\n",
    "    min_learning_rate: float = 1e-5\n",
    "    scheduler_period: int = 10  # cosine-annealing period (epochs)\n",
    "    mixed_precision: bool = True  # enable AMP when CUDA is available\n",
    "    segmentation_positive_weight: float = 1.5  # kept for potential focal weighting experiments\n",
    "    grad_clip_norm: Optional[float] = 5.0  # guard against exploding gradients\n",
    "    imagenet_mean: Tuple[float, float, float] = (0.485, 0.456, 0.406)\n",
    "    imagenet_std: Tuple[float, float, float] = (0.229, 0.224, 0.225)\n",
    "    segmentation_suffix: str = \"_segmentation\"\n",
    "    mixed_dataset_root: Optional[Path] = None\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        if self.mixed_dataset_root is None:\n",
    "            object.__setattr__(self, \"mixed_dataset_root\", self.project_root / self.mixed_dataset_name)\n",
    "        if not self.mixed_dataset_root.exists():\n",
    "            raise FileNotFoundError(\n",
    "                f\"Expected mixed dataset directory at {self.mixed_dataset_root}, but it was not found.\"\n",
    "            )\n",
    "\n",
    "    def manifest_path(self, subset: str) -> Path:\n",
    "        \"\"\"Return the manifest CSV for a given subset inside the mixed dataset.\"\"\"\n",
    "        path = self.mixed_dataset_root / subset / \"manifest.csv\"\n",
    "        if not path.exists():\n",
    "            raise FileNotFoundError(f\"Manifest for subset '{subset}' not found at {path}\")\n",
    "        return path\n",
    "\n",
    "    def resolve_mixed_path(self, relative_path: str) -> Path:\n",
    "        \"\"\"Resolve a manifest path (stored relative to the mixed dataset root) to an absolute Path.\"\"\"\n",
    "        abs_path = self.mixed_dataset_root / Path(relative_path)\n",
    "        if not abs_path.exists():\n",
    "            raise FileNotFoundError(f\"Resolved path does not exist: {abs_path}\")\n",
    "        return abs_path\n",
    "    \n",
    "cfg = Config()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3538c31",
   "metadata": {},
   "source": [
    "# Image utilites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c7e5c664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transforms\n",
    "def build_classification_transform(cfg: Config, train: bool) -> transforms.Compose:\n",
    "    \"\"\"Compose the augmentations used for the image-level classifier.\"\"\"\n",
    "    augmentations: List[transforms.Compose]\n",
    "    if train:\n",
    "        augmentations = [\n",
    "            # Random crops and flips mimic the appearance variability described in the paper\n",
    "            transforms.RandomResizedCrop(cfg.image_size, scale=(0.8, 1.0), interpolation=InterpolationMode.BILINEAR),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomVerticalFlip(p=0.1),\n",
    "            transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n",
    "        ]\n",
    "    else:\n",
    "        augmentations = [\n",
    "            transforms.Resize(cfg.image_size, interpolation=InterpolationMode.BILINEAR),\n",
    "            transforms.CenterCrop(cfg.image_size),\n",
    "        ]\n",
    "    augmentations += [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=cfg.imagenet_mean, std=cfg.imagenet_std),\n",
    "    ]\n",
    "    return transforms.Compose(augmentations)\n",
    "\n",
    "\n",
    "def apply_segmentation_transforms(\n",
    "    image: Image.Image, mask: Image.Image, cfg: Config, train: bool\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Apply the paper's spatial augmentations jointly and emit integer masks for cross-entropy.\"\"\"\n",
    "    image = image.convert(\"RGB\")\n",
    "    mask = mask.convert(\"L\")\n",
    "    if train:\n",
    "        if random.random() < 0.5:\n",
    "            image = TF.hflip(image)\n",
    "            mask = TF.hflip(mask)\n",
    "        if random.random() < 0.2:\n",
    "            image = TF.vflip(image)\n",
    "            mask = TF.vflip(mask)\n",
    "        if random.random() < 0.3:\n",
    "            angle = random.uniform(-15.0, 15.0)\n",
    "            image = TF.rotate(image, angle, interpolation=InterpolationMode.BILINEAR)\n",
    "            mask = TF.rotate(mask, angle, interpolation=InterpolationMode.NEAREST)\n",
    "    image = TF.resize(image, cfg.image_size, interpolation=InterpolationMode.BILINEAR)\n",
    "    mask = TF.resize(mask, cfg.image_size, interpolation=InterpolationMode.NEAREST)\n",
    "    image_tensor = TF.normalize(TF.to_tensor(image), mean=cfg.imagenet_mean, std=cfg.imagenet_std)\n",
    "    # Binarise lesion area then convert to class indices (0 background, 1 lesion) for pixel-wise cross-entropy\n",
    "    mask_array = (np.array(mask, dtype=np.uint8) > 0).astype(np.int64)\n",
    "    mask_tensor = torch.from_numpy(mask_array)\n",
    "    return image_tensor, mask_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa6edad",
   "metadata": {},
   "source": [
    "# Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162e7981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset definitions\n",
    "class ISICClassificationDataset(Dataset):\n",
    "    \"\"\"Loads image-level labels used for the global diagnostic head.\"\"\"\n",
    "\n",
    "    def __init__(self, cfg: Config, split: str):\n",
    "        self.cfg = cfg\n",
    "        self.split = split\n",
    "        self.transform = build_classification_transform(self.cfg, self.split == \"train\")\n",
    "        label_columns = list(cfg.class_names)\n",
    "        manifests: List[pd.DataFrame] = []\n",
    "\n",
    "        for subset in (\"paired\", \"classification_only\"):\n",
    "            manifest_path = cfg.manifest_path(subset)\n",
    "            subset_df = pd.read_csv(manifest_path)\n",
    "            if \"split\" not in subset_df.columns:\n",
    "                raise KeyError(f\"Manifest at {manifest_path} is missing a 'split' column\")\n",
    "            subset_df = subset_df[subset_df[\"split\"] == split].copy()\n",
    "            if subset_df.empty:\n",
    "                continue\n",
    "            subset_df[\"image_abs\"] = subset_df[\"image\"].apply(lambda rel: str(cfg.resolve_mixed_path(rel)))\n",
    "            subset_df[\"image_id\"] = subset_df[\"image\"].apply(lambda rel: Path(rel).stem)\n",
    "            manifests.append(subset_df)\n",
    "\n",
    "        if not manifests:\n",
    "            raise RuntimeError(f\"No classification samples found for split '{split}' in the mixed dataset\")\n",
    "\n",
    "        self.metadata = pd.concat(manifests, ignore_index=True)\n",
    "        missing_cols = [col for col in label_columns if col not in self.metadata.columns]\n",
    "        if missing_cols:\n",
    "            raise RuntimeError(\n",
    "                \"Classification manifest is missing expected label columns: \" + \", \".join(missing_cols)\n",
    "            )\n",
    "        self.label_vectors = self.metadata[label_columns].values.astype(np.float32)\n",
    "        self.image_paths = [Path(path) for path in self.metadata[\"image_abs\"]]\n",
    "        self.image_ids = self.metadata[\"image_id\"].tolist()\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Dict[str, torch.Tensor]:\n",
    "        image_path = self.image_paths[index]\n",
    "        if not image_path.exists():\n",
    "            raise FileNotFoundError(f\"Classification image not found at {image_path}\")\n",
    "        image = Image.open(image_path)\n",
    "        tensor = self.transform(image)\n",
    "        label_vector = self.label_vectors[index]\n",
    "        label = torch.tensor(int(label_vector.argmax()), dtype=torch.long)\n",
    "        return {\n",
    "            \"image\": tensor,\n",
    "            \"label\": label,\n",
    "            \"label_one_hot\": torch.from_numpy(label_vector),\n",
    "            \"image_id\": self.image_ids[index],\n",
    "        }\n",
    "\n",
    "\n",
    "class ISICSegmentationDataset(Dataset):\n",
    "    \"\"\"Provides pixel-level annotations consumed by the decoder/fusion branches.\"\"\"\n",
    "\n",
    "    def __init__(self, cfg: Config, split: str):\n",
    "        self.cfg = cfg\n",
    "        self.split = split\n",
    "        manifests: List[pd.DataFrame] = []\n",
    "\n",
    "        for subset in (\"paired\", \"segmentation_only\"):\n",
    "            manifest_path = cfg.manifest_path(subset)\n",
    "            subset_df = pd.read_csv(manifest_path)\n",
    "            if \"split\" not in subset_df.columns:\n",
    "                raise KeyError(f\"Manifest at {manifest_path} is missing a 'split' column\")\n",
    "            subset_df = subset_df[subset_df[\"split\"] == split].copy()\n",
    "            if subset_df.empty:\n",
    "                continue\n",
    "            subset_df[\"image_abs\"] = subset_df[\"image\"].apply(lambda rel: str(cfg.resolve_mixed_path(rel)))\n",
    "            subset_df[\"mask_abs\"] = subset_df[\"mask\"].apply(lambda rel: str(cfg.resolve_mixed_path(rel)))\n",
    "            subset_df[\"image_id\"] = subset_df[\"image\"].apply(lambda rel: Path(rel).stem)\n",
    "            manifests.append(subset_df)\n",
    "\n",
    "        if not manifests:\n",
    "            raise RuntimeError(f\"No segmentation samples found for split '{split}' in the mixed dataset\")\n",
    "\n",
    "        self.metadata = pd.concat(manifests, ignore_index=True)\n",
    "        self.image_paths = [Path(path) for path in self.metadata[\"image_abs\"]]\n",
    "        self.mask_paths = [Path(path) for path in self.metadata[\"mask_abs\"]]\n",
    "        self.image_ids = self.metadata[\"image_id\"].tolist()\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Dict[str, torch.Tensor]:\n",
    "        image_path = self.image_paths[index]\n",
    "        mask_path = self.mask_paths[index]\n",
    "        if not image_path.exists():\n",
    "            raise FileNotFoundError(f\"Segmentation image not found at {image_path}\")\n",
    "        if not mask_path.exists():\n",
    "            raise FileNotFoundError(f\"Segmentation mask not found at {mask_path}\")\n",
    "        image = Image.open(image_path)\n",
    "        mask = Image.open(mask_path)\n",
    "        image_tensor, mask_tensor = apply_segmentation_transforms(\n",
    "            image, mask, self.cfg, train=self.split == \"train\"\n",
    "        )\n",
    "        return {\n",
    "            \"image\": image_tensor,\n",
    "            \"mask\": mask_tensor,\n",
    "            \"image_id\": self.image_ids[index],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9347fa1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaders created succesfully\n"
     ]
    }
   ],
   "source": [
    "# DataModule-style helpers\n",
    "def create_datasets(cfg: Config) -> Tuple[Dict[str, Dataset], Dict[str, Dataset]]:\n",
    "    \"\"\"Instantiate train/val/test splits for both tasks.\"\"\"\n",
    "    classification = {split: ISICClassificationDataset(cfg, split) for split in (\"train\", \"val\", \"test\")}\n",
    "    segmentation = {split: ISICSegmentationDataset(cfg, split) for split in (\"train\", \"val\", \"test\")}\n",
    "    return classification, segmentation\n",
    "\n",
    "\n",
    "def create_dataloaders(\n",
    "    cfg: Config,\n",
    "    classification: Dict[str, Dataset],\n",
    "    segmentation: Dict[str, Dataset],\n",
    ") -> Tuple[Dict[str, DataLoader], Dict[str, DataLoader]]:\n",
    "    \"\"\"Wrap datasets in PyTorch dataloaders with paper-inspired batching.\"\"\"\n",
    "    classification_loaders = {\n",
    "        split: DataLoader(\n",
    "            dataset,\n",
    "            batch_size=cfg.batch_size,\n",
    "            shuffle=split == \"train\",\n",
    "            drop_last=split == \"train\"\n",
    "        )\n",
    "        for split, dataset in classification.items()\n",
    "    }\n",
    "    segmentation_loaders = {\n",
    "        split: DataLoader(\n",
    "            dataset,\n",
    "            batch_size=cfg.segmentation_batch_size,\n",
    "            shuffle=split == \"train\",\n",
    "            drop_last=split == \"train\"\n",
    "        )\n",
    "        for split, dataset in segmentation.items()\n",
    "    }\n",
    "    return classification_loaders, segmentation_loaders\n",
    "\n",
    "classification, segmentation = create_datasets(cfg)\n",
    "classification_loader, segmentation_loader = create_dataloaders(cfg, classification, segmentation) \n",
    "# loaders are dict of the form {train: train_loader, val: val_loader, test: test_loader}\n",
    "# The print acts as a quick sanity check mirroring dataset stats shown in the paper\n",
    "print(\"Loaders created succesfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea17f400",
   "metadata": {},
   "source": [
    "## Dataset sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a19d611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 2594, 'val': 100, 'test': 1000}\n",
      "{'train': 2594, 'val': 100, 'test': 1000}\n",
      "Classification sample: torch.Size([3, 256, 256]) tensor([0., 1., 0., 0., 0., 0., 0.]) train/input/ISIC_0000000.jpg\n",
      "Segmentation sample: torch.Size([3, 256, 256]) torch.Size([256, 256]) ISIC_0000000\n"
     ]
    }
   ],
   "source": [
    "# Quick dataset sanity check\n",
    "classification_datasets, segmentation_datasets = create_datasets(cfg)\n",
    "# Mirror the counts reported in the appendix (ensures file structure is correct)\n",
    "print({split: len(ds) for split, ds in classification_datasets.items()})\n",
    "print({split: len(ds) for split, ds in segmentation_datasets.items()})\n",
    "sample_cls = classification_datasets[\"train\"][0]\n",
    "sample_seg = segmentation_datasets[\"train\"][0]\n",
    "print(\"Classification sample:\", sample_cls[\"image\"].shape, sample_cls[\"label_one_hot\"], sample_cls[\"image_id\"])\n",
    "print(\"Segmentation sample:\", sample_seg[\"image\"].shape, sample_seg[\"mask\"].shape, sample_seg[\"image_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c7cc82",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f77b875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-task model skeleton with ResNet-50 backbone, dual heads, and stacked cross-fusion modules\n",
    "class ConvBNReLU(nn.Module):\n",
    "    \"\"\"Utility block repeatedly used in decoder refinements (convolution + BN + ReLU).\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int = 3, stride: int = 1):\n",
    "        super().__init__()  # register parameters and buffers\n",
    "        padding = kernel_size // 2  # preserve spatial size for odd kernels to keep skip connections aligned\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),  # stabilise feature statistics while training\n",
    "            nn.ReLU(inplace=True),  # lightweight non-linearity reusing storage\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class ClassificationBranch(nn.Module):\n",
    "    \"\"\"Fully connected classifier with four hidden layers and dropout regularization.\"\"\"\n",
    "\n",
    "    def __init__(self, in_features: int, hidden_dims: Tuple[int, int, int, int], num_classes: int):\n",
    "        super().__init__()\n",
    "        if len(hidden_dims) != 4:\n",
    "            raise ValueError(\"hidden_dims must contain four entries for the extended branch\")\n",
    "        self.hidden_dims = hidden_dims  # expose dims for cross-fusion wiring\n",
    "        dims = (in_features,) + hidden_dims\n",
    "        # Construct sequential linear layers that can be selectively invoked after each fusion stage\n",
    "        self.layers = nn.ModuleList(\n",
    "            nn.Linear(dims[i], dims[i + 1]) for i in range(len(hidden_dims))\n",
    "        )\n",
    "        self.dropout = nn.Dropout(p=0.3)  # moderate rate to discourage overfitting\n",
    "        self.head = nn.Linear(hidden_dims[-1], num_classes)  # final logits for frame-level classification\n",
    "\n",
    "    def activate_layer(self, x: torch.Tensor, idx: int) -> torch.Tensor:\n",
    "        \"\"\"Apply the idx-th hidden layer followed by ReLU and dropout.\"\"\"\n",
    "        if idx < 0 or idx >= len(self.layers):\n",
    "            raise IndexError(\"Layer index out of range for classification branch\")\n",
    "        x = self.layers[idx](x)\n",
    "        x = F.relu(x)\n",
    "        return self.dropout(x)\n",
    "\n",
    "    def final_logits(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Compute classification logits from the last hidden representation.\"\"\"\n",
    "        return self.head(x)\n",
    "\n",
    "\n",
    "class SegmentationBranch(nn.Module):\n",
    "    \"\"\"Image-to-image decoder with additional refinement blocks for deeper fusion points.\"\"\"\n",
    "\n",
    "    def __init__(self, encoder_channels: Tuple[int, int, int, int, int], out_channels: int):\n",
    "        super().__init__()\n",
    "        c0, c1, c2, c3, c4 = encoder_channels\n",
    "        # Standard U-Net style upsampling path: progressively merge encoder features with decoder activations\n",
    "        self.reduce = ConvBNReLU(c4, 512, kernel_size=1)  # compress deepest features before upsampling\n",
    "        self.up3 = ConvBNReLU(512 + c3, 256)\n",
    "        self.up2 = ConvBNReLU(256 + c2, 128)\n",
    "        self.up1 = ConvBNReLU(128 + c1, 96)\n",
    "        self.up0 = ConvBNReLU(96 + c0, 64)\n",
    "        # Refinement convolutions let fusion interactions happen on stable feature maps\n",
    "        self.refine1 = ConvBNReLU(64, 64)  # refinement prior to second fusion\n",
    "        self.refine2 = ConvBNReLU(64, 64)  # refinement prior to prediction\n",
    "        # Produce background/lesion logits so a softmax classifier can score each pixel\n",
    "        self.prediction = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "\n",
    "    def decode(self, features: Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]) -> torch.Tensor:\n",
    "        \"\"\"Run the entire top-down decoder and return the high-resolution feature map for fusion.\"\"\"\n",
    "        c0, c1, c2, c3, c4 = features\n",
    "        x = self.reduce(c4)\n",
    "        x = F.interpolate(x, size=c3.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "        x = self.up3(torch.cat([x, c3], dim=1))\n",
    "        x = F.interpolate(x, size=c2.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "        x = self.up2(torch.cat([x, c2], dim=1))\n",
    "        x = F.interpolate(x, size=c1.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "        x = self.up1(torch.cat([x, c1], dim=1))\n",
    "        x = F.interpolate(x, size=c0.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "        return self.up0(torch.cat([x, c0], dim=1))  # high-res feature map used for fusion\n",
    "\n",
    "    def predict(self, features: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Project refined features to per-pixel logits (softmax later gives class probabilities).\"\"\"\n",
    "        return self.prediction(features)\n",
    "\n",
    "\n",
    "class CrossFusionModule(nn.Module):\n",
    "    \"\"\"Bidirectional cross-fusion: couples classification (global) and segmentation (dense) features.\"\"\"\n",
    "\n",
    "    def __init__(self, cls_channels: int, seg_channels: int):\n",
    "        super().__init__()\n",
    "        # Shared transformation matrix M from the paper (Eq. 7) implemented as a 1×1 convolution\n",
    "        self.transform = nn.Conv2d(seg_channels, cls_channels, kernel_size=1, bias=False)\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)  # used in the segmentation→classification path\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        cls_feature: torch.Tensor,  # shape (batch, cls_channels, 1, 1)\n",
    "        seg_feature: torch.Tensor,  # shape (batch, seg_channels, H, W)\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # X'_cls = X_cls + Pool(M(X_seg)) — Eq. (7a) converts dense activations to a global descriptor\n",
    "        seg_to_cls = self.pool(self.transform(seg_feature))\n",
    "        fused_cls = cls_feature + seg_to_cls\n",
    "\n",
    "        # X'_seg = X_seg + M^T(Pad(X_cls)) — Eq. (7b) injects classification context into the decoder\n",
    "        h, w = seg_feature.shape[-2:]\n",
    "        cls_expanded = cls_feature.expand(-1, cls_feature.size(1), h, w)  # Pad(X_cls) for spatial compatibility\n",
    "        cls_to_seg = F.conv_transpose2d(cls_expanded, self.transform.weight)  # transpose convolution applies M^T\n",
    "        fused_seg = seg_feature + cls_to_seg\n",
    "        return fused_cls, fused_seg\n",
    "\n",
    "\n",
    "class MultiTask(nn.Module):\n",
    "    \"\"\"Multi-task architecture with shared ResNet-50 encoder and stacked cross-fusion heads.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int,\n",
    "        num_segmentation_classes: int = 2,\n",
    "        trainable_backbone_layers: int = 2,\n",
    "        use_pretrained: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        weights = ResNet50_Weights.DEFAULT if use_pretrained else None\n",
    "        backbone = resnet50(weights=weights)\n",
    "\n",
    "        # Slice ResNet-50 to expose stage outputs for skip connections and frozen fine-tuning control\n",
    "        self.stem = nn.Sequential(backbone.conv1, backbone.bn1, backbone.relu)\n",
    "        self.maxpool = backbone.maxpool\n",
    "        self.layer1 = backbone.layer1\n",
    "        self.layer2 = backbone.layer2\n",
    "        self.layer3 = backbone.layer3\n",
    "        self.layer4 = backbone.layer4\n",
    "        self._set_trainable_layers(trainable_backbone_layers)\n",
    "\n",
    "        encoder_channels = (64, 256, 512, 1024, 2048)  # channel counts produced by each encoder stage\n",
    "        classification_hidden_dims = (1024, 512, 256, 128)  # width of the four FC layers\n",
    "        self.classifier_pool = nn.AdaptiveAvgPool2d(1)  # compress encoder output before the MLP\n",
    "        self.classifier_branch = ClassificationBranch(\n",
    "            in_features=2048, hidden_dims=classification_hidden_dims, num_classes=num_classes\n",
    "        )\n",
    "        self.segmentation_branch = SegmentationBranch(\n",
    "            encoder_channels=encoder_channels, out_channels=num_segmentation_classes\n",
    "        )\n",
    "\n",
    "        # Two cross-fusion modules reproduce the coarse-to-fine interaction proposed in the paper\n",
    "        self.cross_fusion_primary = CrossFusionModule(\n",
    "            cls_channels=2048, seg_channels=64\n",
    "        )\n",
    "        self.cross_fusion_secondary = CrossFusionModule(\n",
    "            cls_channels=self.classifier_branch.hidden_dims[0], seg_channels=64\n",
    "        )\n",
    "\n",
    "    def _set_trainable_layers(self, trainable_backbone_layers: int) -> None:\n",
    "        \"\"\"Freeze early ResNet blocks to control how much of the backbone is fine-tuned.\"\"\"\n",
    "        if trainable_backbone_layers < 1 or trainable_backbone_layers > 5:\n",
    "            raise ValueError(\"trainable_backbone_layers must be between 1 and 5\")\n",
    "        stages = [self.stem, self.layer1, self.layer2, self.layer3, self.layer4]\n",
    "        trainable = stages[-trainable_backbone_layers:]  # choose the last N stages to keep trainable\n",
    "        for module in stages:\n",
    "            requires_grad = module in trainable\n",
    "            for param in module.parameters():\n",
    "                param.requires_grad = requires_grad\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        task: Optional[str] = None,\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Run shared encoder, two-stage cross-fusion, and emit requested task predictions.\"\"\"\n",
    "        if task and task not in {\"classification\", \"segmentation\"}:\n",
    "            raise ValueError(\"task must be 'classification', 'segmentation', or None\")\n",
    "\n",
    "        # Shared ResNet-50 encoder produces multi-scale features for both tasks\n",
    "        c0 = self.stem(x)\n",
    "        p0 = self.maxpool(c0)\n",
    "        c1 = self.layer1(p0)\n",
    "        c2 = self.layer2(c1)\n",
    "        c3 = self.layer3(c2)\n",
    "        c4 = self.layer4(c3)\n",
    "        encoder_features = (c0, c1, c2, c3, c4)\n",
    "\n",
    "        # Stage 1 — coarse fusion (Fig. 3): share information between deepest decoder map and global embedding\n",
    "        seg_stage0 = self.segmentation_branch.decode(encoder_features)\n",
    "        cls_stage0 = self.classifier_pool(c4)  # (batch, 2048, 1, 1) global descriptor\n",
    "        cls_stage1, seg_stage1 = self.cross_fusion_primary(cls_stage0, seg_stage0)\n",
    "\n",
    "        # Track reconstruction penalties Σ MSE(X_i', X_i) used in the paper's Eq. (11)\n",
    "        fusion_penalties: List[torch.Tensor] = []\n",
    "        fusion_penalties.append(F.mse_loss(cls_stage1, cls_stage0, reduction=\"mean\"))\n",
    "        fusion_penalties.append(F.mse_loss(seg_stage1, seg_stage0, reduction=\"mean\"))\n",
    "\n",
    "        # Stage 2 — mid-level fusion: combine first FC layer with refined decoder feature map\n",
    "        cls_vector = cls_stage1.flatten(1)  # collapse pooled tensor into vector for FC layer\n",
    "        cls_hidden1 = self.classifier_branch.activate_layer(cls_vector, idx=0)  # first hidden representation\n",
    "        cls_hidden1_map = cls_hidden1.view(cls_hidden1.size(0), cls_hidden1.size(1), 1, 1)  # reshape for cross fusion\n",
    "        seg_stage1_refined = self.segmentation_branch.refine1(seg_stage1)\n",
    "        cls_stage2, seg_stage2 = self.cross_fusion_secondary(cls_hidden1_map, seg_stage1_refined)\n",
    "        fusion_penalties.append(F.mse_loss(cls_stage2, cls_hidden1_map, reduction=\"mean\"))\n",
    "        fusion_penalties.append(F.mse_loss(seg_stage2, seg_stage1_refined, reduction=\"mean\"))\n",
    "\n",
    "        # Finish classification path: run remaining dense layers and compute logits\n",
    "        cls_stage2_flat = cls_stage2.flatten(1)\n",
    "        cls_hidden2 = self.classifier_branch.activate_layer(cls_stage2_flat, idx=1)\n",
    "        cls_hidden3 = self.classifier_branch.activate_layer(cls_hidden2, idx=2)\n",
    "        cls_hidden4 = self.classifier_branch.activate_layer(cls_hidden3, idx=3)\n",
    "        cls_logits = self.classifier_branch.final_logits(cls_hidden4)\n",
    "\n",
    "        # Finish segmentation path: refine with residual conv and produce segmentation logits\n",
    "        seg_stage2_refined = self.segmentation_branch.refine2(seg_stage2)\n",
    "        seg_logits = self.segmentation_branch.predict(seg_stage2_refined)\n",
    "        seg_logits = F.interpolate(seg_logits, size=x.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "        fusion_reg = torch.stack(fusion_penalties).sum()\n",
    "\n",
    "        outputs: Dict[str, torch.Tensor] = {}\n",
    "        requested = {task} if task else {\"classification\", \"segmentation\"}\n",
    "        if \"classification\" in requested:\n",
    "            outputs[\"classification\"] = cls_logits\n",
    "            outputs[\"fusion_reg\"] = fusion_reg\n",
    "        if \"segmentation\" in requested:\n",
    "            outputs[\"segmentation\"] = seg_logits\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b2387499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Losses and metrics\n",
    "def build_loss_functions(cfg: Config, device: torch.device) -> Tuple[nn.Module, nn.Module]:\n",
    "    \"\"\"Prepare task-specific objectives used during optimisation.\"\"\"\n",
    "    # Classification: standard cross-entropy over lesion categories\n",
    "    classification_loss = nn.CrossEntropyLoss()\n",
    "    # Segmentation: pixel-wise cross-entropy between softmax logits and ground-truth class map\n",
    "    segmentation_loss = nn.CrossEntropyLoss(ignore_index=cfg.ignore_index)\n",
    "    return classification_loss, segmentation_loss\n",
    "\n",
    "\n",
    "def classification_accuracy(logits: torch.Tensor, labels: torch.Tensor) -> float:\n",
    "    \"\"\"Compute top-1 accuracy for the classification branch.\"\"\"\n",
    "    predictions = logits.argmax(dim=1)\n",
    "    correct = (predictions == labels).sum().item()\n",
    "    total = labels.numel()\n",
    "    return correct / max(total, 1)\n",
    "\n",
    "\n",
    "def dice_score(logits: torch.Tensor, targets: torch.Tensor, threshold: float = 0.5, eps: float = 1e-6) -> float:\n",
    "    \"\"\"Compute Dice overlap for the lesion class using softmax probabilities.\"\"\"\n",
    "    probabilities = torch.softmax(logits, dim=1)[:, 1, ...]  # lesion channel\n",
    "    preds = (probabilities > threshold).float()\n",
    "    targets = targets.float()\n",
    "    intersection = (preds * targets).sum(dim=(1, 2))\n",
    "    union = preds.sum(dim=(1, 2)) + targets.sum(dim=(1, 2))\n",
    "    dice = (2 * intersection + eps) / (union + eps)\n",
    "    return dice.mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423063c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training utilities\n",
    "def create_optimizer(model: nn.Module, cfg: Config) -> Tuple[torch.optim.Optimizer, CosineAnnealingLR]:\n",
    "    \"\"\"Build AdamW optimizer and cosine scheduler for the parameters left trainable.\"\"\"\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.AdamW(params, lr=cfg.base_learning_rate, weight_decay=cfg.weight_decay)\n",
    "    scheduler = CosineAnnealingLR(\n",
    "        optimizer, T_max=cfg.scheduler_period, eta_min=cfg.min_learning_rate\n",
    "    )\n",
    "    return optimizer, scheduler\n",
    "\n",
    "\n",
    "def _next_batch(\n",
    "    iterator: Iterator[Dict[str, torch.Tensor]], loader: DataLoader\n",
    ") -> Tuple[Dict[str, torch.Tensor], Iterator[Dict[str, torch.Tensor]]]:\n",
    "    \"\"\"Fetch the next batch and restart the iterator when the dataloader is exhausted.\"\"\"\n",
    "    try:\n",
    "        batch = next(iterator)\n",
    "    except StopIteration:\n",
    "        iterator = iter(loader)\n",
    "        batch = next(iterator)\n",
    "    return batch, iterator\n",
    "\n",
    "\n",
    "def train_one_epoch(\n",
    "    model: nn.Module,\n",
    "    classification_loader: DataLoader,\n",
    "    segmentation_loader: DataLoader,\n",
    "    classification_loss_fn: nn.Module,\n",
    "    segmentation_loss_fn: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    cfg: Config,\n",
    "    device: torch.device,\n",
    "    scaler: Optional[GradScaler] = None,\n",
    " ) -> Dict[str, float]:\n",
    "    \"\"\"Train the network for a single epoch while logging both task metrics.\"\"\"\n",
    "    model.train()\n",
    "    use_amp = scaler is not None and scaler.is_enabled()\n",
    "    classification_iter = iter(classification_loader)\n",
    "    segmentation_iter = iter(segmentation_loader)\n",
    "    max_steps = max(len(classification_loader), len(segmentation_loader))\n",
    "    total_loss = 0.0\n",
    "    total_cls_loss = 0.0\n",
    "    total_seg_loss = 0.0\n",
    "    total_accuracy = 0.0\n",
    "    total_dice = 0.0\n",
    "    for step in range(max_steps):\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        classification_batch, classification_iter = _next_batch(classification_iter, classification_loader)\n",
    "        segmentation_batch, segmentation_iter = _next_batch(segmentation_iter, segmentation_loader)\n",
    "        classification_images = classification_batch[\"image\"].to(device, non_blocking=True)\n",
    "        classification_labels = classification_batch[\"label\"].to(device, non_blocking=True)\n",
    "        segmentation_images = segmentation_batch[\"image\"].to(device, non_blocking=True)\n",
    "        segmentation_masks = segmentation_batch[\"mask\"].to(device, non_blocking=True).long()\n",
    "        amp_context = autocast() if use_amp and device.type == \"cuda\" else nullcontext()\n",
    "        with amp_context:\n",
    "            # Classification branch: cross-entropy plus α-weighted fusion regularizer\n",
    "            classification_result = model(classification_images, task=\"classification\")\n",
    "            cls_logits = classification_result[\"classification\"]\n",
    "            fusion_reg = classification_result.get(\"fusion_reg\")\n",
    "            if fusion_reg is None:\n",
    "                fusion_reg = cls_logits.new_zeros(())\n",
    "            cls_loss = classification_loss_fn(cls_logits, classification_labels) + cfg.alpha * fusion_reg\n",
    "            # Segmentation branch: pixel-wise cross-entropy between logits and integer mask\n",
    "            segmentation_result = model(segmentation_images, task=\"segmentation\")\n",
    "            seg_logits = segmentation_result[\"segmentation\"]\n",
    "            seg_loss = segmentation_loss_fn(seg_logits, segmentation_masks)\n",
    "            loss = cfg.classification_loss_weight * cls_loss + cfg.segmentation_loss_weight * seg_loss\n",
    "        if use_amp and device.type == \"cuda\":\n",
    "            scaler.scale(loss).backward()\n",
    "            if cfg.grad_clip_norm and cfg.grad_clip_norm > 0:\n",
    "                scaler.unscale_(optimizer)\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip_norm)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            if cfg.grad_clip_norm and cfg.grad_clip_norm > 0:\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip_norm)\n",
    "            optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        total_cls_loss += cls_loss.item()\n",
    "        total_seg_loss += seg_loss.item()\n",
    "        total_accuracy += classification_accuracy(cls_logits.detach(), classification_labels)\n",
    "        total_dice += dice_score(seg_logits.detach(), segmentation_masks)\n",
    "    steps = float(max_steps)\n",
    "    return {\n",
    "        \"loss\": total_loss / steps,\n",
    "        \"classification_loss\": total_cls_loss / steps,\n",
    "        \"segmentation_loss\": total_seg_loss / steps,\n",
    "        \"classification_accuracy\": total_accuracy / steps,\n",
    "        \"segmentation_dice\": total_dice / steps,\n",
    "    }\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(\n",
    "    model: nn.Module,\n",
    "    classification_loader: DataLoader,\n",
    "    segmentation_loader: DataLoader,\n",
    "    classification_loss_fn: nn.Module,\n",
    "    segmentation_loss_fn: nn.Module,\n",
    "    cfg: Config,\n",
    "    device: torch.device,\n",
    " ) -> Dict[str, float]:\n",
    "    \"\"\"Run validation/testing without gradient tracking.\"\"\"\n",
    "    model.eval()\n",
    "    cls_loss_total = 0.0\n",
    "    cls_acc_total = 0.0\n",
    "    cls_steps = 0\n",
    "    for batch in classification_loader:\n",
    "        images = batch[\"image\"].to(device, non_blocking=True)\n",
    "        labels = batch[\"label\"].to(device, non_blocking=True)\n",
    "        classification_result = model(images, task=\"classification\")\n",
    "        cls_logits = classification_result[\"classification\"]\n",
    "        fusion_reg = classification_result.get(\"fusion_reg\")\n",
    "        if fusion_reg is None:\n",
    "            fusion_reg = cls_logits.new_zeros(())\n",
    "        cls_loss = classification_loss_fn(cls_logits, labels) + cfg.alpha * fusion_reg\n",
    "        cls_loss_total += cls_loss.item()\n",
    "        cls_acc_total += classification_accuracy(cls_logits, labels)\n",
    "        cls_steps += 1\n",
    "    seg_loss_total = 0.0\n",
    "    seg_dice_total = 0.0\n",
    "    seg_steps = 0\n",
    "    for batch in segmentation_loader:\n",
    "        images = batch[\"image\"].to(device, non_blocking=True)\n",
    "        masks = batch[\"mask\"].to(device, non_blocking=True).long()\n",
    "        segmentation_result = model(images, task=\"segmentation\")\n",
    "        seg_logits = segmentation_result[\"segmentation\"]\n",
    "        seg_loss_total += segmentation_loss_fn(seg_logits, masks).item()\n",
    "        seg_dice_total += dice_score(seg_logits, masks)\n",
    "        seg_steps += 1\n",
    "    cls_steps = max(cls_steps, 1)\n",
    "    seg_steps = max(seg_steps, 1)\n",
    "    return {\n",
    "        \"classification_loss\": cls_loss_total / cls_steps,\n",
    "        \"classification_accuracy\": cls_acc_total / cls_steps,\n",
    "        \"segmentation_loss\": seg_loss_total / seg_steps,\n",
    "        \"segmentation_dice\": seg_dice_total / seg_steps,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97cef6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# High-level training loop\n",
    "def fit(\n",
    "    cfg: Config,\n",
    "    device: torch.device,\n",
    "    output_dir: Optional[Path] = None,\n",
    "    resume_from: Optional[Path] = None,\n",
    "    save_checkpoints: bool = True,\n",
    ") -> Dict[str, List[Dict[str, float]]]:\n",
    "    classification_datasets, segmentation_datasets = create_datasets(cfg)\n",
    "    classification_loaders, segmentation_loaders = create_dataloaders(\n",
    "        cfg, classification_datasets, segmentation_datasets\n",
    "    )\n",
    "    model = MultiTask(num_classes=len(cfg.class_names)).to(device)\n",
    "    classification_loss_fn, segmentation_loss_fn = build_loss_functions(cfg, device)\n",
    "    optimizer, scheduler = create_optimizer(model, cfg)\n",
    "    use_amp = cfg.mixed_precision and device.type == \"cuda\"\n",
    "    scaler = GradScaler(enabled=use_amp) if use_amp else None\n",
    "    start_epoch = 0\n",
    "    best_val_dice = 0.0\n",
    "    history: Dict[str, List[Dict[str, float]]] = {\"train\": [], \"val\": []}\n",
    "    if output_dir:\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    if resume_from:\n",
    "        checkpoint = torch.load(resume_from, map_location=device)\n",
    "        model.load_state_dict(checkpoint[\"model\"])\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "        scheduler.load_state_dict(checkpoint[\"scheduler\"])\n",
    "        if scaler and \"scaler\" in checkpoint:\n",
    "            scaler.load_state_dict(checkpoint[\"scaler\"])\n",
    "        start_epoch = checkpoint.get(\"epoch\", 0) + 1\n",
    "        best_val_dice = checkpoint.get(\"best_val_dice\", best_val_dice)\n",
    "        print(f\"Resumed training from {resume_from} at epoch {start_epoch}\")\n",
    "    for epoch in range(start_epoch, cfg.max_epochs):\n",
    "        train_metrics = train_one_epoch(\n",
    "            model,\n",
    "            classification_loaders[\"train\"],\n",
    "            segmentation_loaders[\"train\"],\n",
    "            classification_loss_fn,\n",
    "            segmentation_loss_fn,\n",
    "            optimizer,\n",
    "            cfg,\n",
    "            device,\n",
    "            scaler,\n",
    "        )\n",
    "        val_metrics = evaluate(\n",
    "            model,\n",
    "            classification_loaders[\"val\"],\n",
    "            segmentation_loaders[\"val\"],\n",
    "            classification_loss_fn,\n",
    "            segmentation_loss_fn,\n",
    "            cfg,\n",
    "            device,\n",
    "        )\n",
    "        scheduler.step()\n",
    "        history[\"train\"].append(train_metrics)\n",
    "        history[\"val\"].append(val_metrics)\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}/{cfg.max_epochs} | \"\n",
    "            f\"Train Loss: {train_metrics['loss']:.4f} | \"\n",
    "            f\"Val Acc: {val_metrics['classification_accuracy']:.4f} | \"\n",
    "            f\"Val Dice: {val_metrics['segmentation_dice']:.4f}\"\n",
    "        )\n",
    "        current_val_dice = val_metrics[\"segmentation_dice\"]\n",
    "        if save_checkpoints and output_dir:\n",
    "            checkpoint = {\n",
    "                \"epoch\": epoch,\n",
    "                \"model\": model.state_dict(),\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "                \"scheduler\": scheduler.state_dict(),\n",
    "                \"cfg\": cfg.__dict__,\n",
    "                \"train_metrics\": train_metrics,\n",
    "                \"val_metrics\": val_metrics,\n",
    "                \"best_val_dice\": max(best_val_dice, current_val_dice),\n",
    "            }\n",
    "            if scaler:\n",
    "                checkpoint[\"scaler\"] = scaler.state_dict()\n",
    "            torch.save(checkpoint, output_dir / f\"multitask_resnet50_epoch_{epoch + 1:03d}.pth\")\n",
    "        if current_val_dice > best_val_dice:\n",
    "            best_val_dice = current_val_dice\n",
    "    model.eval()\n",
    "    return {\n",
    "        \"history\": history,\n",
    "        \"model\": model,\n",
    "        \"classification_loaders\": classification_loaders,\n",
    "        \"segmentation_loaders\": segmentation_loaders,\n",
    "        \"best_val_dice\": best_val_dice,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6e0aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example training call (disabled by default)\n",
    "# results = fit(\n",
    "#     cfg,\n",
    "#     device,\n",
    "#     output_dir=Path(\"artifacts/multitask\"),\n",
    "#     resume_from=None,\n",
    "#     save_checkpoints=True,\n",
    "# )\n",
    "# best_model = results[\"model\"]\n",
    "# history = results[\"history\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8743dee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved untrained weights to multitask_resnet50.pth\n"
     ]
    }
   ],
   "source": [
    "# Save an untrained model checkpoint\n",
    "model = MultiTask(num_classes=len(cfg.class_names)).to(\"cpu\")\n",
    "save_path = Path(\"multitask_resnet50.pth\")\n",
    "save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"Saved untrained weights to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0587af3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "============================================================================================================================================\n",
       "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #                   Trainable\n",
       "============================================================================================================================================\n",
       "MultiTask                                [1, 3, 256, 256]          [1, 1, 128, 128]          --                        Partial\n",
       "├─Sequential: 1-1                        [1, 3, 256, 256]          [1, 64, 128, 128]         --                        False\n",
       "│    └─Conv2d: 2-1                       [1, 3, 256, 256]          [1, 64, 128, 128]         (9,408)                   False\n",
       "│    └─BatchNorm2d: 2-2                  [1, 64, 128, 128]         [1, 64, 128, 128]         (128)                     False\n",
       "│    └─ReLU: 2-3                         [1, 64, 128, 128]         [1, 64, 128, 128]         --                        --\n",
       "├─MaxPool2d: 1-2                         [1, 64, 128, 128]         [1, 64, 64, 64]           --                        --\n",
       "├─Sequential: 1-3                        [1, 64, 64, 64]           [1, 256, 64, 64]          --                        False\n",
       "│    └─Bottleneck: 2-4                   [1, 64, 64, 64]           [1, 256, 64, 64]          --                        False\n",
       "│    │    └─Conv2d: 3-1                  [1, 64, 64, 64]           [1, 64, 64, 64]           (4,096)                   False\n",
       "│    │    └─BatchNorm2d: 3-2             [1, 64, 64, 64]           [1, 64, 64, 64]           (128)                     False\n",
       "│    │    └─ReLU: 3-3                    [1, 64, 64, 64]           [1, 64, 64, 64]           --                        --\n",
       "│    │    └─Conv2d: 3-4                  [1, 64, 64, 64]           [1, 64, 64, 64]           (36,864)                  False\n",
       "│    │    └─BatchNorm2d: 3-5             [1, 64, 64, 64]           [1, 64, 64, 64]           (128)                     False\n",
       "│    │    └─ReLU: 3-6                    [1, 64, 64, 64]           [1, 64, 64, 64]           --                        --\n",
       "│    │    └─Conv2d: 3-7                  [1, 64, 64, 64]           [1, 256, 64, 64]          (16,384)                  False\n",
       "│    │    └─BatchNorm2d: 3-8             [1, 256, 64, 64]          [1, 256, 64, 64]          (512)                     False\n",
       "│    │    └─Sequential: 3-9              [1, 64, 64, 64]           [1, 256, 64, 64]          (16,896)                  False\n",
       "│    │    └─ReLU: 3-10                   [1, 256, 64, 64]          [1, 256, 64, 64]          --                        --\n",
       "│    └─Bottleneck: 2-5                   [1, 256, 64, 64]          [1, 256, 64, 64]          --                        False\n",
       "│    │    └─Conv2d: 3-11                 [1, 256, 64, 64]          [1, 64, 64, 64]           (16,384)                  False\n",
       "│    │    └─BatchNorm2d: 3-12            [1, 64, 64, 64]           [1, 64, 64, 64]           (128)                     False\n",
       "│    │    └─ReLU: 3-13                   [1, 64, 64, 64]           [1, 64, 64, 64]           --                        --\n",
       "│    │    └─Conv2d: 3-14                 [1, 64, 64, 64]           [1, 64, 64, 64]           (36,864)                  False\n",
       "│    │    └─BatchNorm2d: 3-15            [1, 64, 64, 64]           [1, 64, 64, 64]           (128)                     False\n",
       "│    │    └─ReLU: 3-16                   [1, 64, 64, 64]           [1, 64, 64, 64]           --                        --\n",
       "│    │    └─Conv2d: 3-17                 [1, 64, 64, 64]           [1, 256, 64, 64]          (16,384)                  False\n",
       "│    │    └─BatchNorm2d: 3-18            [1, 256, 64, 64]          [1, 256, 64, 64]          (512)                     False\n",
       "│    │    └─ReLU: 3-19                   [1, 256, 64, 64]          [1, 256, 64, 64]          --                        --\n",
       "│    └─Bottleneck: 2-6                   [1, 256, 64, 64]          [1, 256, 64, 64]          --                        False\n",
       "│    │    └─Conv2d: 3-20                 [1, 256, 64, 64]          [1, 64, 64, 64]           (16,384)                  False\n",
       "│    │    └─BatchNorm2d: 3-21            [1, 64, 64, 64]           [1, 64, 64, 64]           (128)                     False\n",
       "│    │    └─ReLU: 3-22                   [1, 64, 64, 64]           [1, 64, 64, 64]           --                        --\n",
       "│    │    └─Conv2d: 3-23                 [1, 64, 64, 64]           [1, 64, 64, 64]           (36,864)                  False\n",
       "│    │    └─BatchNorm2d: 3-24            [1, 64, 64, 64]           [1, 64, 64, 64]           (128)                     False\n",
       "│    │    └─ReLU: 3-25                   [1, 64, 64, 64]           [1, 64, 64, 64]           --                        --\n",
       "│    │    └─Conv2d: 3-26                 [1, 64, 64, 64]           [1, 256, 64, 64]          (16,384)                  False\n",
       "│    │    └─BatchNorm2d: 3-27            [1, 256, 64, 64]          [1, 256, 64, 64]          (512)                     False\n",
       "│    │    └─ReLU: 3-28                   [1, 256, 64, 64]          [1, 256, 64, 64]          --                        --\n",
       "├─Sequential: 1-4                        [1, 256, 64, 64]          [1, 512, 32, 32]          --                        False\n",
       "│    └─Bottleneck: 2-7                   [1, 256, 64, 64]          [1, 512, 32, 32]          --                        False\n",
       "│    │    └─Conv2d: 3-29                 [1, 256, 64, 64]          [1, 128, 64, 64]          (32,768)                  False\n",
       "│    │    └─BatchNorm2d: 3-30            [1, 128, 64, 64]          [1, 128, 64, 64]          (256)                     False\n",
       "│    │    └─ReLU: 3-31                   [1, 128, 64, 64]          [1, 128, 64, 64]          --                        --\n",
       "│    │    └─Conv2d: 3-32                 [1, 128, 64, 64]          [1, 128, 32, 32]          (147,456)                 False\n",
       "│    │    └─BatchNorm2d: 3-33            [1, 128, 32, 32]          [1, 128, 32, 32]          (256)                     False\n",
       "│    │    └─ReLU: 3-34                   [1, 128, 32, 32]          [1, 128, 32, 32]          --                        --\n",
       "│    │    └─Conv2d: 3-35                 [1, 128, 32, 32]          [1, 512, 32, 32]          (65,536)                  False\n",
       "│    │    └─BatchNorm2d: 3-36            [1, 512, 32, 32]          [1, 512, 32, 32]          (1,024)                   False\n",
       "│    │    └─Sequential: 3-37             [1, 256, 64, 64]          [1, 512, 32, 32]          (132,096)                 False\n",
       "│    │    └─ReLU: 3-38                   [1, 512, 32, 32]          [1, 512, 32, 32]          --                        --\n",
       "│    └─Bottleneck: 2-8                   [1, 512, 32, 32]          [1, 512, 32, 32]          --                        False\n",
       "│    │    └─Conv2d: 3-39                 [1, 512, 32, 32]          [1, 128, 32, 32]          (65,536)                  False\n",
       "│    │    └─BatchNorm2d: 3-40            [1, 128, 32, 32]          [1, 128, 32, 32]          (256)                     False\n",
       "│    │    └─ReLU: 3-41                   [1, 128, 32, 32]          [1, 128, 32, 32]          --                        --\n",
       "│    │    └─Conv2d: 3-42                 [1, 128, 32, 32]          [1, 128, 32, 32]          (147,456)                 False\n",
       "│    │    └─BatchNorm2d: 3-43            [1, 128, 32, 32]          [1, 128, 32, 32]          (256)                     False\n",
       "│    │    └─ReLU: 3-44                   [1, 128, 32, 32]          [1, 128, 32, 32]          --                        --\n",
       "│    │    └─Conv2d: 3-45                 [1, 128, 32, 32]          [1, 512, 32, 32]          (65,536)                  False\n",
       "│    │    └─BatchNorm2d: 3-46            [1, 512, 32, 32]          [1, 512, 32, 32]          (1,024)                   False\n",
       "│    │    └─ReLU: 3-47                   [1, 512, 32, 32]          [1, 512, 32, 32]          --                        --\n",
       "│    └─Bottleneck: 2-9                   [1, 512, 32, 32]          [1, 512, 32, 32]          --                        False\n",
       "│    │    └─Conv2d: 3-48                 [1, 512, 32, 32]          [1, 128, 32, 32]          (65,536)                  False\n",
       "│    │    └─BatchNorm2d: 3-49            [1, 128, 32, 32]          [1, 128, 32, 32]          (256)                     False\n",
       "│    │    └─ReLU: 3-50                   [1, 128, 32, 32]          [1, 128, 32, 32]          --                        --\n",
       "│    │    └─Conv2d: 3-51                 [1, 128, 32, 32]          [1, 128, 32, 32]          (147,456)                 False\n",
       "│    │    └─BatchNorm2d: 3-52            [1, 128, 32, 32]          [1, 128, 32, 32]          (256)                     False\n",
       "│    │    └─ReLU: 3-53                   [1, 128, 32, 32]          [1, 128, 32, 32]          --                        --\n",
       "│    │    └─Conv2d: 3-54                 [1, 128, 32, 32]          [1, 512, 32, 32]          (65,536)                  False\n",
       "│    │    └─BatchNorm2d: 3-55            [1, 512, 32, 32]          [1, 512, 32, 32]          (1,024)                   False\n",
       "│    │    └─ReLU: 3-56                   [1, 512, 32, 32]          [1, 512, 32, 32]          --                        --\n",
       "│    └─Bottleneck: 2-10                  [1, 512, 32, 32]          [1, 512, 32, 32]          --                        False\n",
       "│    │    └─Conv2d: 3-57                 [1, 512, 32, 32]          [1, 128, 32, 32]          (65,536)                  False\n",
       "│    │    └─BatchNorm2d: 3-58            [1, 128, 32, 32]          [1, 128, 32, 32]          (256)                     False\n",
       "│    │    └─ReLU: 3-59                   [1, 128, 32, 32]          [1, 128, 32, 32]          --                        --\n",
       "│    │    └─Conv2d: 3-60                 [1, 128, 32, 32]          [1, 128, 32, 32]          (147,456)                 False\n",
       "│    │    └─BatchNorm2d: 3-61            [1, 128, 32, 32]          [1, 128, 32, 32]          (256)                     False\n",
       "│    │    └─ReLU: 3-62                   [1, 128, 32, 32]          [1, 128, 32, 32]          --                        --\n",
       "│    │    └─Conv2d: 3-63                 [1, 128, 32, 32]          [1, 512, 32, 32]          (65,536)                  False\n",
       "│    │    └─BatchNorm2d: 3-64            [1, 512, 32, 32]          [1, 512, 32, 32]          (1,024)                   False\n",
       "│    │    └─ReLU: 3-65                   [1, 512, 32, 32]          [1, 512, 32, 32]          --                        --\n",
       "├─Sequential: 1-5                        [1, 512, 32, 32]          [1, 1024, 16, 16]         --                        True\n",
       "│    └─Bottleneck: 2-11                  [1, 512, 32, 32]          [1, 1024, 16, 16]         --                        True\n",
       "│    │    └─Conv2d: 3-66                 [1, 512, 32, 32]          [1, 256, 32, 32]          131,072                   True\n",
       "│    │    └─BatchNorm2d: 3-67            [1, 256, 32, 32]          [1, 256, 32, 32]          512                       True\n",
       "│    │    └─ReLU: 3-68                   [1, 256, 32, 32]          [1, 256, 32, 32]          --                        --\n",
       "│    │    └─Conv2d: 3-69                 [1, 256, 32, 32]          [1, 256, 16, 16]          589,824                   True\n",
       "│    │    └─BatchNorm2d: 3-70            [1, 256, 16, 16]          [1, 256, 16, 16]          512                       True\n",
       "│    │    └─ReLU: 3-71                   [1, 256, 16, 16]          [1, 256, 16, 16]          --                        --\n",
       "│    │    └─Conv2d: 3-72                 [1, 256, 16, 16]          [1, 1024, 16, 16]         262,144                   True\n",
       "│    │    └─BatchNorm2d: 3-73            [1, 1024, 16, 16]         [1, 1024, 16, 16]         2,048                     True\n",
       "│    │    └─Sequential: 3-74             [1, 512, 32, 32]          [1, 1024, 16, 16]         526,336                   True\n",
       "│    │    └─ReLU: 3-75                   [1, 1024, 16, 16]         [1, 1024, 16, 16]         --                        --\n",
       "│    └─Bottleneck: 2-12                  [1, 1024, 16, 16]         [1, 1024, 16, 16]         --                        True\n",
       "│    │    └─Conv2d: 3-76                 [1, 1024, 16, 16]         [1, 256, 16, 16]          262,144                   True\n",
       "│    │    └─BatchNorm2d: 3-77            [1, 256, 16, 16]          [1, 256, 16, 16]          512                       True\n",
       "│    │    └─ReLU: 3-78                   [1, 256, 16, 16]          [1, 256, 16, 16]          --                        --\n",
       "│    │    └─Conv2d: 3-79                 [1, 256, 16, 16]          [1, 256, 16, 16]          589,824                   True\n",
       "│    │    └─BatchNorm2d: 3-80            [1, 256, 16, 16]          [1, 256, 16, 16]          512                       True\n",
       "│    │    └─ReLU: 3-81                   [1, 256, 16, 16]          [1, 256, 16, 16]          --                        --\n",
       "│    │    └─Conv2d: 3-82                 [1, 256, 16, 16]          [1, 1024, 16, 16]         262,144                   True\n",
       "│    │    └─BatchNorm2d: 3-83            [1, 1024, 16, 16]         [1, 1024, 16, 16]         2,048                     True\n",
       "│    │    └─ReLU: 3-84                   [1, 1024, 16, 16]         [1, 1024, 16, 16]         --                        --\n",
       "│    └─Bottleneck: 2-13                  [1, 1024, 16, 16]         [1, 1024, 16, 16]         --                        True\n",
       "│    │    └─Conv2d: 3-85                 [1, 1024, 16, 16]         [1, 256, 16, 16]          262,144                   True\n",
       "│    │    └─BatchNorm2d: 3-86            [1, 256, 16, 16]          [1, 256, 16, 16]          512                       True\n",
       "│    │    └─ReLU: 3-87                   [1, 256, 16, 16]          [1, 256, 16, 16]          --                        --\n",
       "│    │    └─Conv2d: 3-88                 [1, 256, 16, 16]          [1, 256, 16, 16]          589,824                   True\n",
       "│    │    └─BatchNorm2d: 3-89            [1, 256, 16, 16]          [1, 256, 16, 16]          512                       True\n",
       "│    │    └─ReLU: 3-90                   [1, 256, 16, 16]          [1, 256, 16, 16]          --                        --\n",
       "│    │    └─Conv2d: 3-91                 [1, 256, 16, 16]          [1, 1024, 16, 16]         262,144                   True\n",
       "│    │    └─BatchNorm2d: 3-92            [1, 1024, 16, 16]         [1, 1024, 16, 16]         2,048                     True\n",
       "│    │    └─ReLU: 3-93                   [1, 1024, 16, 16]         [1, 1024, 16, 16]         --                        --\n",
       "│    └─Bottleneck: 2-14                  [1, 1024, 16, 16]         [1, 1024, 16, 16]         --                        True\n",
       "│    │    └─Conv2d: 3-94                 [1, 1024, 16, 16]         [1, 256, 16, 16]          262,144                   True\n",
       "│    │    └─BatchNorm2d: 3-95            [1, 256, 16, 16]          [1, 256, 16, 16]          512                       True\n",
       "│    │    └─ReLU: 3-96                   [1, 256, 16, 16]          [1, 256, 16, 16]          --                        --\n",
       "│    │    └─Conv2d: 3-97                 [1, 256, 16, 16]          [1, 256, 16, 16]          589,824                   True\n",
       "│    │    └─BatchNorm2d: 3-98            [1, 256, 16, 16]          [1, 256, 16, 16]          512                       True\n",
       "│    │    └─ReLU: 3-99                   [1, 256, 16, 16]          [1, 256, 16, 16]          --                        --\n",
       "│    │    └─Conv2d: 3-100                [1, 256, 16, 16]          [1, 1024, 16, 16]         262,144                   True\n",
       "│    │    └─BatchNorm2d: 3-101           [1, 1024, 16, 16]         [1, 1024, 16, 16]         2,048                     True\n",
       "│    │    └─ReLU: 3-102                  [1, 1024, 16, 16]         [1, 1024, 16, 16]         --                        --\n",
       "│    └─Bottleneck: 2-15                  [1, 1024, 16, 16]         [1, 1024, 16, 16]         --                        True\n",
       "│    │    └─Conv2d: 3-103                [1, 1024, 16, 16]         [1, 256, 16, 16]          262,144                   True\n",
       "│    │    └─BatchNorm2d: 3-104           [1, 256, 16, 16]          [1, 256, 16, 16]          512                       True\n",
       "│    │    └─ReLU: 3-105                  [1, 256, 16, 16]          [1, 256, 16, 16]          --                        --\n",
       "│    │    └─Conv2d: 3-106                [1, 256, 16, 16]          [1, 256, 16, 16]          589,824                   True\n",
       "│    │    └─BatchNorm2d: 3-107           [1, 256, 16, 16]          [1, 256, 16, 16]          512                       True\n",
       "│    │    └─ReLU: 3-108                  [1, 256, 16, 16]          [1, 256, 16, 16]          --                        --\n",
       "│    │    └─Conv2d: 3-109                [1, 256, 16, 16]          [1, 1024, 16, 16]         262,144                   True\n",
       "│    │    └─BatchNorm2d: 3-110           [1, 1024, 16, 16]         [1, 1024, 16, 16]         2,048                     True\n",
       "│    │    └─ReLU: 3-111                  [1, 1024, 16, 16]         [1, 1024, 16, 16]         --                        --\n",
       "│    └─Bottleneck: 2-16                  [1, 1024, 16, 16]         [1, 1024, 16, 16]         --                        True\n",
       "│    │    └─Conv2d: 3-112                [1, 1024, 16, 16]         [1, 256, 16, 16]          262,144                   True\n",
       "│    │    └─BatchNorm2d: 3-113           [1, 256, 16, 16]          [1, 256, 16, 16]          512                       True\n",
       "│    │    └─ReLU: 3-114                  [1, 256, 16, 16]          [1, 256, 16, 16]          --                        --\n",
       "│    │    └─Conv2d: 3-115                [1, 256, 16, 16]          [1, 256, 16, 16]          589,824                   True\n",
       "│    │    └─BatchNorm2d: 3-116           [1, 256, 16, 16]          [1, 256, 16, 16]          512                       True\n",
       "│    │    └─ReLU: 3-117                  [1, 256, 16, 16]          [1, 256, 16, 16]          --                        --\n",
       "│    │    └─Conv2d: 3-118                [1, 256, 16, 16]          [1, 1024, 16, 16]         262,144                   True\n",
       "│    │    └─BatchNorm2d: 3-119           [1, 1024, 16, 16]         [1, 1024, 16, 16]         2,048                     True\n",
       "│    │    └─ReLU: 3-120                  [1, 1024, 16, 16]         [1, 1024, 16, 16]         --                        --\n",
       "├─Sequential: 1-6                        [1, 1024, 16, 16]         [1, 2048, 8, 8]           --                        True\n",
       "│    └─Bottleneck: 2-17                  [1, 1024, 16, 16]         [1, 2048, 8, 8]           --                        True\n",
       "│    │    └─Conv2d: 3-121                [1, 1024, 16, 16]         [1, 512, 16, 16]          524,288                   True\n",
       "│    │    └─BatchNorm2d: 3-122           [1, 512, 16, 16]          [1, 512, 16, 16]          1,024                     True\n",
       "│    │    └─ReLU: 3-123                  [1, 512, 16, 16]          [1, 512, 16, 16]          --                        --\n",
       "│    │    └─Conv2d: 3-124                [1, 512, 16, 16]          [1, 512, 8, 8]            2,359,296                 True\n",
       "│    │    └─BatchNorm2d: 3-125           [1, 512, 8, 8]            [1, 512, 8, 8]            1,024                     True\n",
       "│    │    └─ReLU: 3-126                  [1, 512, 8, 8]            [1, 512, 8, 8]            --                        --\n",
       "│    │    └─Conv2d: 3-127                [1, 512, 8, 8]            [1, 2048, 8, 8]           1,048,576                 True\n",
       "│    │    └─BatchNorm2d: 3-128           [1, 2048, 8, 8]           [1, 2048, 8, 8]           4,096                     True\n",
       "│    │    └─Sequential: 3-129            [1, 1024, 16, 16]         [1, 2048, 8, 8]           2,101,248                 True\n",
       "│    │    └─ReLU: 3-130                  [1, 2048, 8, 8]           [1, 2048, 8, 8]           --                        --\n",
       "│    └─Bottleneck: 2-18                  [1, 2048, 8, 8]           [1, 2048, 8, 8]           --                        True\n",
       "│    │    └─Conv2d: 3-131                [1, 2048, 8, 8]           [1, 512, 8, 8]            1,048,576                 True\n",
       "│    │    └─BatchNorm2d: 3-132           [1, 512, 8, 8]            [1, 512, 8, 8]            1,024                     True\n",
       "│    │    └─ReLU: 3-133                  [1, 512, 8, 8]            [1, 512, 8, 8]            --                        --\n",
       "│    │    └─Conv2d: 3-134                [1, 512, 8, 8]            [1, 512, 8, 8]            2,359,296                 True\n",
       "│    │    └─BatchNorm2d: 3-135           [1, 512, 8, 8]            [1, 512, 8, 8]            1,024                     True\n",
       "│    │    └─ReLU: 3-136                  [1, 512, 8, 8]            [1, 512, 8, 8]            --                        --\n",
       "│    │    └─Conv2d: 3-137                [1, 512, 8, 8]            [1, 2048, 8, 8]           1,048,576                 True\n",
       "│    │    └─BatchNorm2d: 3-138           [1, 2048, 8, 8]           [1, 2048, 8, 8]           4,096                     True\n",
       "│    │    └─ReLU: 3-139                  [1, 2048, 8, 8]           [1, 2048, 8, 8]           --                        --\n",
       "│    └─Bottleneck: 2-19                  [1, 2048, 8, 8]           [1, 2048, 8, 8]           --                        True\n",
       "│    │    └─Conv2d: 3-140                [1, 2048, 8, 8]           [1, 512, 8, 8]            1,048,576                 True\n",
       "│    │    └─BatchNorm2d: 3-141           [1, 512, 8, 8]            [1, 512, 8, 8]            1,024                     True\n",
       "│    │    └─ReLU: 3-142                  [1, 512, 8, 8]            [1, 512, 8, 8]            --                        --\n",
       "│    │    └─Conv2d: 3-143                [1, 512, 8, 8]            [1, 512, 8, 8]            2,359,296                 True\n",
       "│    │    └─BatchNorm2d: 3-144           [1, 512, 8, 8]            [1, 512, 8, 8]            1,024                     True\n",
       "│    │    └─ReLU: 3-145                  [1, 512, 8, 8]            [1, 512, 8, 8]            --                        --\n",
       "│    │    └─Conv2d: 3-146                [1, 512, 8, 8]            [1, 2048, 8, 8]           1,048,576                 True\n",
       "│    │    └─BatchNorm2d: 3-147           [1, 2048, 8, 8]           [1, 2048, 8, 8]           4,096                     True\n",
       "│    │    └─ReLU: 3-148                  [1, 2048, 8, 8]           [1, 2048, 8, 8]           --                        --\n",
       "├─SegmentationBranch: 1-14               --                        --                        (recursive)               True\n",
       "│    └─ConvBNReLU: 2-20                  [1, 2048, 8, 8]           [1, 512, 8, 8]            --                        True\n",
       "│    │    └─Sequential: 3-149            [1, 2048, 8, 8]           [1, 512, 8, 8]            1,049,600                 True\n",
       "│    └─ConvBNReLU: 2-21                  [1, 1536, 16, 16]         [1, 256, 16, 16]          --                        True\n",
       "│    │    └─Sequential: 3-150            [1, 1536, 16, 16]         [1, 256, 16, 16]          3,539,456                 True\n",
       "│    └─ConvBNReLU: 2-22                  [1, 768, 32, 32]          [1, 128, 32, 32]          --                        True\n",
       "│    │    └─Sequential: 3-151            [1, 768, 32, 32]          [1, 128, 32, 32]          884,992                   True\n",
       "│    └─ConvBNReLU: 2-23                  [1, 384, 64, 64]          [1, 96, 64, 64]           --                        True\n",
       "│    │    └─Sequential: 3-152            [1, 384, 64, 64]          [1, 96, 64, 64]           331,968                   True\n",
       "│    └─ConvBNReLU: 2-24                  [1, 160, 128, 128]        [1, 64, 128, 128]         --                        True\n",
       "│    │    └─Sequential: 3-153            [1, 160, 128, 128]        [1, 64, 128, 128]         92,288                    True\n",
       "├─AdaptiveAvgPool2d: 1-8                 [1, 2048, 8, 8]           [1, 2048, 1, 1]           --                        --\n",
       "├─CrossFusionModule: 1-9                 [1, 2048, 1, 1]           [1, 2048, 1, 1]           --                        True\n",
       "│    └─Conv2d: 2-25                      [1, 64, 128, 128]         [1, 2048, 128, 128]       131,072                   True\n",
       "│    └─AdaptiveAvgPool2d: 2-26           [1, 2048, 128, 128]       [1, 2048, 1, 1]           --                        --\n",
       "├─ClassificationBranch: 1-13             --                        --                        (recursive)               True\n",
       "│    └─ModuleList: 2-36                  --                        --                        (recursive)               True\n",
       "│    │    └─Linear: 3-154                [1, 2048]                 [1, 1024]                 2,098,176                 True\n",
       "│    └─Dropout: 2-28                     [1, 1024]                 [1, 1024]                 --                        --\n",
       "├─SegmentationBranch: 1-14               --                        --                        (recursive)               True\n",
       "│    └─ConvBNReLU: 2-29                  [1, 64, 128, 128]         [1, 64, 128, 128]         --                        True\n",
       "│    │    └─Sequential: 3-155            [1, 64, 128, 128]         [1, 64, 128, 128]         36,992                    True\n",
       "├─CrossFusionModule: 1-12                [1, 1024, 1, 1]           [1, 1024, 1, 1]           --                        True\n",
       "│    └─Conv2d: 2-30                      [1, 64, 128, 128]         [1, 1024, 128, 128]       65,536                    True\n",
       "│    └─AdaptiveAvgPool2d: 2-31           [1, 1024, 128, 128]       [1, 1024, 1, 1]           --                        --\n",
       "├─ClassificationBranch: 1-13             --                        --                        (recursive)               True\n",
       "│    └─ModuleList: 2-36                  --                        --                        (recursive)               True\n",
       "│    │    └─Linear: 3-156                [1, 1024]                 [1, 512]                  524,800                   True\n",
       "│    └─Dropout: 2-33                     [1, 512]                  [1, 512]                  --                        --\n",
       "│    └─ModuleList: 2-36                  --                        --                        (recursive)               True\n",
       "│    │    └─Linear: 3-157                [1, 512]                  [1, 256]                  131,328                   True\n",
       "│    └─Dropout: 2-35                     [1, 256]                  [1, 256]                  --                        --\n",
       "│    └─ModuleList: 2-36                  --                        --                        (recursive)               True\n",
       "│    │    └─Linear: 3-158                [1, 256]                  [1, 128]                  32,896                    True\n",
       "│    └─Dropout: 2-37                     [1, 128]                  [1, 128]                  --                        --\n",
       "│    └─Linear: 2-38                      [1, 128]                  [1, 7]                    903                       True\n",
       "├─SegmentationBranch: 1-14               --                        --                        (recursive)               True\n",
       "│    └─ConvBNReLU: 2-39                  [1, 64, 128, 128]         [1, 64, 128, 128]         --                        True\n",
       "│    │    └─Sequential: 3-159            [1, 64, 128, 128]         [1, 64, 128, 128]         36,992                    True\n",
       "│    └─Conv2d: 2-40                      [1, 64, 128, 128]         [1, 1, 128, 128]          65                        True\n",
       "============================================================================================================================================\n",
       "Total params: 32,465,096\n",
       "Trainable params: 31,020,168\n",
       "Non-trainable params: 1,444,928\n",
       "Total mult-adds (Units.GIGABYTES): 14.52\n",
       "============================================================================================================================================\n",
       "Input size (MB): 0.79\n",
       "Forward/backward pass size (MB): 695.35\n",
       "Params size (MB): 129.86\n",
       "Estimated Total Size (MB): 826.00\n",
       "============================================================================================================================================"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "model = MultiTask(num_classes=len(cfg.class_names))\n",
    "summary(model, input_size=(1, 3, *cfg.image_size), col_names=(\"input_size\", \"output_size\", \"num_params\", \"trainable\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46eb4fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the best_model captured during training on the held-out test splits\n",
    "if \"best_model\" not in globals():\n",
    "    raise RuntimeError(\"best_model is undefined. Run the training cell that returns best_model before inference.\")\n",
    "model = best_model.to(device).eval()\n",
    "classification_datasets, segmentation_datasets = create_datasets(cfg)\n",
    "classification_loaders, segmentation_loaders = create_dataloaders(\n",
    "    cfg, classification_datasets, segmentation_datasets\n",
    ")\n",
    "classification_loss_fn, segmentation_loss_fn = build_loss_functions(cfg, device)\n",
    "test_metrics = evaluate(\n",
    "    model,\n",
    "    classification_loaders[\"test\"],\n",
    "    segmentation_loaders[\"test\"],\n",
    "    classification_loss_fn,\n",
    "    segmentation_loss_fn,\n",
    "    cfg,\n",
    "    device,\n",
    ")\n",
    "print(\"Test metrics:\")\n",
    "for key, value in test_metrics.items():\n",
    "    print(f\"  {key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411492fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece13c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise model predictions on a random test sample\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "if \"best_model\" not in globals():\n",
    "    raise RuntimeError(\"Run training to populate best_model before visualisation.\")\n",
    "model = best_model.to(device).eval()\n",
    "\n",
    "# Ensure test datasets/loaders are available\n",
    "if \"classification_datasets\" not in globals() or \"segmentation_datasets\" not in globals():\n",
    "    classification_datasets, segmentation_datasets = create_datasets(cfg)\n",
    "if \"classification_loaders\" not in globals() or \"segmentation_loaders\" not in globals():\n",
    "    classification_loaders, segmentation_loaders = create_dataloaders(\n",
    "        cfg, classification_datasets, segmentation_datasets\n",
    "    )\n",
    "\n",
    "test_segmentation_dataset = segmentation_datasets[\"test\"]\n",
    "sample_idx = random.randrange(len(test_segmentation_dataset))\n",
    "sample = test_segmentation_dataset[sample_idx]\n",
    "image = sample[\"image\"].unsqueeze(0).to(device)\n",
    "mask_gt = sample[\"mask\"].numpy()\n",
    "image_id = sample[\"image_id\"]\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(image)\n",
    "    cls_logits = outputs[\"classification\"]\n",
    "    seg_logits = outputs[\"segmentation\"]\n",
    "    cls_probs = torch.softmax(cls_logits, dim=1)[0].cpu().numpy()\n",
    "    pred_class_idx = int(cls_probs.argmax())\n",
    "    pred_class_conf = float(cls_probs[pred_class_idx])\n",
    "    pred_class_label = cfg.class_names[pred_class_idx]\n",
    "    seg_probs = torch.softmax(seg_logits, dim=1)[0, 1].cpu().numpy()\n",
    "\n",
    "# Try to recover the ground-truth classification label (if the image exists in the CSV)\n",
    "ground_truth_label = \"N/A\"\n",
    "test_cls_dataset = classification_datasets[\"test\"]\n",
    "filename_candidate = f\"{image_id}.jpg\"\n",
    "try:\n",
    "    cls_index = test_cls_dataset.image_paths.index(filename_candidate)\n",
    "    label_vector = test_cls_dataset.label_vectors[cls_index]\n",
    "    gt_idx = int(label_vector.argmax())\n",
    "    ground_truth_label = cfg.class_names[gt_idx]\n",
    "except ValueError:\n",
    "    pass\n",
    "\n",
    "# Denormalise image tensor for visualisation\n",
    "mean = np.array(cfg.imagenet_mean)\n",
    "std = np.array(cfg.imagenet_std)\n",
    "image_np = sample[\"image\"].permute(1, 2, 0).cpu().numpy()\n",
    "image_np = (image_np * std) + mean\n",
    "image_np = np.clip(image_np, 0.0, 1.0)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "axes[0].imshow(image_np)\n",
    "axes[0].set_title(f\"Input image: {image_id}\")\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "axes[1].imshow(mask_gt, cmap=\"gray\")\n",
    "axes[1].set_title(\"Ground-truth mask\")\n",
    "axes[1].axis(\"off\")\n",
    "\n",
    "axes[2].imshow(image_np)\n",
    "axes[2].imshow(seg_probs, cmap=\"viridis\", alpha=0.5)\n",
    "axes[2].set_title(\"Predicted mask overlay\")\n",
    "axes[2].axis(\"off\")\n",
    "\n",
    "plt.suptitle(\n",
    "    f\"Predicted label: {pred_class_label} (p={pred_class_conf:.2f}) | Ground truth: {ground_truth_label}\"\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
