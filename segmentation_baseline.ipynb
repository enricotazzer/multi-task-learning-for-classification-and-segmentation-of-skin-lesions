{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3574f300",
   "metadata": {},
   "source": [
    "# DeepLabv3-ResNet50 Baseline\n",
    "Build and train a semantic segmentation model using DeepLabv3 with a ResNet-50 backbone on the dermoscopy dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82f6ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.cuda.amp as amp\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional as F\n",
    "from torchvision.models.segmentation import (\n",
    "    deeplabv3_resnet50,\n",
    "    DeepLabV3_ResNet50_Weights,\n",
    " )\n",
    "from torchvision.models import ResNet50_Weights\n",
    "\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0899f56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on mps\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "data_root = Path(\"/Users/enricotazzer/Desktop/multi-task-learning-for-classification-and-segmentation-of-skin-lesions/dataset/segmentation\")\n",
    "train_img_dir = data_root / \"train\" / \"input\"\n",
    "train_mask_dir = data_root / \"train\" / \"ground_truth\"\n",
    "val_img_dir = data_root / \"val\" / \"input\"\n",
    "val_mask_dir = data_root / \"val\" / \"ground_truth\"\n",
    "test_img_dir = data_root / \"test\" / \"input\"\n",
    "test_mask_dir = data_root / \"test\" / \"ground_truth\"\n",
    "required_dirs = [train_img_dir, train_mask_dir, val_img_dir, val_mask_dir]\n",
    "for path in required_dirs:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Missing required directory: {path}\")\n",
    "\n",
    "classes = [\"background\", \"lesion\"]\n",
    "num_classes = len(classes)\n",
    "ignore_index = 255\n",
    "image_size = (256, 256)\n",
    "\n",
    "batch_size = 4\n",
    "num_epochs = 25\n",
    "learning_rate = 5e-4\n",
    "weight_decay = 1e-4\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Training on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2e6fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 2594 | Val samples: 66\n",
      "Test samples: 1000\n"
     ]
    }
   ],
   "source": [
    "# Data pipeline\n",
    "weights = DeepLabV3_ResNet50_Weights.DEFAULT\n",
    "normalize_mean = tuple(float(m) for m in weights.meta.get(\"mean\", (0.485, 0.456, 0.406)))\n",
    "normalize_std = tuple(float(s) for s in weights.meta.get(\"std\", (0.229, 0.224, 0.225)))\n",
    "\n",
    "class SegmentationTransform:\n",
    "    def __init__(self, is_train: bool, image_size: Tuple[int, int], mean: Tuple[float, ...], std: Tuple[float, ...]):\n",
    "        self.is_train = is_train\n",
    "        self.image_size = image_size\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, image: Image.Image, mask: Image.Image) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        if self.is_train:\n",
    "            if random.random() < 0.5:\n",
    "                image = F.hflip(image)\n",
    "                mask = F.hflip(mask)\n",
    "            if random.random() < 0.5:\n",
    "                image = F.vflip(image)\n",
    "                mask = F.vflip(mask)\n",
    "            if random.random() < 0.3:\n",
    "                angle = random.uniform(-15.0, 15.0)\n",
    "                image = F.rotate(image, angle, interpolation=transforms.InterpolationMode.BILINEAR)\n",
    "                mask = F.rotate(mask, angle, interpolation=transforms.InterpolationMode.NEAREST)\n",
    "\n",
    "        image = F.resize(image, self.image_size, interpolation=transforms.InterpolationMode.BILINEAR)\n",
    "        mask = F.resize(mask, self.image_size, interpolation=transforms.InterpolationMode.NEAREST)\n",
    "\n",
    "        image = F.to_tensor(image)\n",
    "        image = F.normalize(image, mean=self.mean, std=self.std)\n",
    "\n",
    "        mask_array = np.array(mask, dtype=np.int64)\n",
    "        mask_tensor = torch.from_numpy(mask_array)\n",
    "        mask_tensor = (mask_tensor > 0).to(torch.int64)\n",
    "        return image, mask_tensor\n",
    "\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, image_dir: Path, mask_dir: Path, transform: SegmentationTransform):\n",
    "        self.image_dir = Path(image_dir)\n",
    "        self.mask_dir = Path(mask_dir)\n",
    "        self.transform = transform\n",
    "        self.image_exts = {\".jpg\", \".jpeg\", \".png\", \".bmp\"}\n",
    "        self.mask_exts = [\".png\", \".jpg\", \".jpeg\", \".bmp\"]\n",
    "        if not self.image_dir.exists():\n",
    "            raise FileNotFoundError(f\"Image directory does not exist: {self.image_dir}\")\n",
    "        if not self.mask_dir.exists():\n",
    "            raise FileNotFoundError(f\"Mask directory does not exist: {self.mask_dir}\")\n",
    "        self.samples = self._gather_samples()\n",
    "        if not self.samples:\n",
    "            raise RuntimeError(f\"No image/mask pairs found in {self.image_dir} and {self.mask_dir}\")\n",
    "\n",
    "    def _gather_samples(self) -> List[Tuple[Path, Path]]:\n",
    "        pairs: List[Tuple[Path, Path]] = []\n",
    "        for image_path in sorted(self.image_dir.iterdir()):\n",
    "            if image_path.suffix.lower() not in self.image_exts:\n",
    "                continue\n",
    "            mask_path = self._find_corresponding_mask(image_path)\n",
    "            pairs.append((image_path, mask_path))\n",
    "        return pairs\n",
    "\n",
    "    def _find_corresponding_mask(self, image_path: Path) -> Path:\n",
    "\n",
    "        stem = image_path.stem+\"_segmentation\"\n",
    "        for ext in self.mask_exts:\n",
    "            candidate = self.mask_dir / f\"{stem}{ext}\"\n",
    "            if candidate.exists():\n",
    "                return candidate\n",
    "        raise FileNotFoundError(f\"Mask for {image_path.name} not found in {self.mask_dir}\")\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        image_path, mask_path = self.samples[index]\n",
    "        with Image.open(image_path) as img:\n",
    "            image = img.convert(\"RGB\")\n",
    "        with Image.open(mask_path) as msk:\n",
    "            mask = msk.convert(\"L\")\n",
    "        image_tensor, mask_tensor = self.transform(image, mask)\n",
    "        return image_tensor, mask_tensor\n",
    "\n",
    "train_transform = SegmentationTransform(is_train=True, image_size=image_size, mean=normalize_mean, std=normalize_std)\n",
    "eval_transform = SegmentationTransform(is_train=False, image_size=image_size, mean=normalize_mean, std=normalize_std)\n",
    "\n",
    "train_dataset = SegmentationDataset(train_img_dir, train_mask_dir, transform=train_transform)\n",
    "val_dataset = SegmentationDataset(val_img_dir, val_mask_dir, transform=eval_transform)\n",
    "test_dataset = (\n",
    "    SegmentationDataset(test_img_dir, test_mask_dir, transform=eval_transform)\n",
    "    if test_img_dir.exists() and test_mask_dir.exists()\n",
    "    else None\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = (\n",
    "    DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    if test_dataset is not None\n",
    "    else None\n",
    ")\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)} | Val samples: {len(val_dataset)}\")\n",
    "if test_dataset is not None:\n",
    "    print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e84729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 39,633,986\n"
     ]
    }
   ],
   "source": [
    "# Model setup\n",
    "model = deeplabv3_resnet50(weights=DeepLabV3_ResNet50_Weights.DEFAULT)\n",
    "model.classifier[-1] = nn.Conv2d(256, num_classes, kernel_size=1)\n",
    "if model.aux_classifier is not None:\n",
    "    model.aux_classifier[-1] = nn.Conv2d(256, num_classes, kernel_size=1)\n",
    "nn.init.xavier_uniform_(model.classifier[-1].weight)\n",
    "nn.init.zeros_(model.classifier[-1].bias)\n",
    "if model.aux_classifier is not None and hasattr(model.aux_classifier[-1], \"bias\") and model.aux_classifier[-1].bias is not None:\n",
    "    nn.init.xavier_uniform_(model.aux_classifier[-1].weight)\n",
    "    nn.init.zeros_(model.aux_classifier[-1].bias)\n",
    "\n",
    "freeze_prefixes = (\"conv1\", \"bn1\", \"layer1\", \"layer2\")\n",
    "for name, param in model.backbone.named_parameters():\n",
    "    if name.split(\".\")[0] in freeze_prefixes:\n",
    "        param.requires_grad = False\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "use_amp = device.type == \"cuda\"\n",
    "scaler = amp.GradScaler(enabled=use_amp)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=ignore_index)\n",
    "trainable_parameters = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.AdamW(trainable_parameters, lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=learning_rate * 0.1)\n",
    "\n",
    "trainable_params = sum(p.numel() for p in trainable_parameters)\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ff20c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training utilities\n",
    "def update_confusion_matrix(confmat: torch.Tensor, preds: torch.Tensor, targets: torch.Tensor, num_classes: int) -> torch.Tensor:\n",
    "    valid = targets != ignore_index\n",
    "    preds = preds[valid]\n",
    "    targets = targets[valid]\n",
    "    if preds.numel() == 0:\n",
    "        return confmat\n",
    "    indices = targets * num_classes + preds\n",
    "    confmat += torch.bincount(indices, minlength=num_classes ** 2).reshape(num_classes, num_classes)\n",
    "    return confmat\n",
    "\n",
    "def compute_iou(confmat: torch.Tensor) -> torch.Tensor:\n",
    "    intersection = torch.diag(confmat)\n",
    "    ground_truth = confmat.sum(dim=1)\n",
    "    predicted = confmat.sum(dim=0)\n",
    "    union = ground_truth + predicted - intersection\n",
    "    iou = intersection / union.clamp(min=1.0)\n",
    "    return iou\n",
    "\n",
    "def train_one_epoch(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    scaler: Optional[amp.GradScaler] = None,\n",
    "    use_amp: bool = False,\n",
    ") -> Tuple[float, float, float]:\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_pixels = 0\n",
    "    total_pixels = 0\n",
    "    start = time.time()\n",
    "\n",
    "    for images, masks in dataloader:\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        masks = masks.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with amp.autocast(enabled=use_amp):\n",
    "            outputs = model(images)[\"out\"]\n",
    "            loss = criterion(outputs, masks)\n",
    "\n",
    "        if scaler is not None and use_amp:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        valid = masks != ignore_index\n",
    "        correct_pixels += (preds[valid] == masks[valid]).sum().item()\n",
    "        total_pixels += valid.sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / max(len(dataloader.dataset), 1)\n",
    "    pixel_acc = correct_pixels / max(total_pixels, 1)\n",
    "    elapsed = time.time() - start\n",
    "    return epoch_loss, pixel_acc, elapsed\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    use_amp: bool = False,\n",
    ") -> Tuple[float, float, float, torch.Tensor]:\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct_pixels = 0\n",
    "    total_pixels = 0\n",
    "    confmat = torch.zeros((num_classes, num_classes), dtype=torch.float64)\n",
    "\n",
    "    for images, masks in dataloader:\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        masks = masks.to(device, non_blocking=True)\n",
    "\n",
    "        with amp.autocast(enabled=use_amp):\n",
    "            outputs = model(images)[\"out\"]\n",
    "            loss = criterion(outputs, masks)\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        valid = masks != ignore_index\n",
    "        correct_pixels += (preds[valid] == masks[valid]).sum().item()\n",
    "        total_pixels += valid.sum().item()\n",
    "\n",
    "        confmat = update_confusion_matrix(confmat, preds.cpu(), masks.cpu(), num_classes)\n",
    "\n",
    "    epoch_loss = running_loss / max(len(dataloader.dataset), 1)\n",
    "    pixel_acc = correct_pixels / max(total_pixels, 1)\n",
    "    per_class_iou = compute_iou(confmat)\n",
    "    if num_classes > 1:\n",
    "        mean_iou = per_class_iou[1:].mean().item()\n",
    "    else:\n",
    "        mean_iou = per_class_iou.mean().item()\n",
    "    return epoch_loss, pixel_acc, mean_iou, per_class_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca8558d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    scheduler: Optional[torch.optim.lr_scheduler._LRScheduler] = None,\n",
    "    num_epochs: int = 25,\n",
    "    checkpoint_dir: str = \"artifacts/segmentation\",\n",
    "    scaler: Optional[amp.GradScaler] = None,\n",
    "    use_amp: bool = False,\n",
    " ) -> Tuple[nn.Module, Dict[str, List[float]]]:\n",
    "    best_state = copy.deepcopy(model.state_dict())\n",
    "    best_miou = 0.0\n",
    "    history: Dict[str, List[float]] = {\n",
    "        \"train_loss\": [],\n",
    "        \"train_pixel_acc\": [],\n",
    "        \"val_loss\": [],\n",
    "        \"val_pixel_acc\": [],\n",
    "        \"val_mIoU\": [],\n",
    "        \"val_per_class_iou\": [],\n",
    "    }\n",
    "\n",
    "    checkpoint_path = Path(checkpoint_dir)\n",
    "    checkpoint_path.mkdir(parents=True, exist_ok=True)\n",
    "    best_ckpt = checkpoint_path / \"deeplabv3_resnet50_best.pt\"\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "        train_loss, train_acc, train_time = train_one_epoch(\n",
    "            model, train_loader, criterion, optimizer, scaler=scaler, use_amp=use_amp\n",
    "        )\n",
    "        val_loss, val_acc, val_miou, val_per_class_iou = evaluate(\n",
    "            model, val_loader, criterion, use_amp=use_amp\n",
    "        )\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_pixel_acc\"].append(train_acc)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_pixel_acc\"].append(val_acc)\n",
    "        history[\"val_mIoU\"].append(val_miou)\n",
    "        history[\"val_per_class_iou\"].append(val_per_class_iou.tolist())\n",
    "\n",
    "        print(\n",
    "            f\"train loss: {train_loss:.4f} | pixel acc: {train_acc:.4f} | time: {train_time:.1f}s\"\n",
    "        )\n",
    "        print(\n",
    "            f\"val   loss: {val_loss:.4f} | pixel acc: {val_acc:.4f} | mIoU: {val_miou:.4f}\"\n",
    "        )\n",
    "\n",
    "        if val_miou > best_miou:\n",
    "            best_miou = val_miou\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "            torch.save({\n",
    "                \"model_state_dict\": best_state,\n",
    "                \"val_mIoU\": best_miou,\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"classes\": classes,\n",
    "                \"image_size\": image_size,\n",
    "                \"per_class_iou\": val_per_class_iou.tolist(),\n",
    "            }, best_ckpt)\n",
    "            print(f\"\\nâœ… Saved new best checkpoint to {best_ckpt}\\n\")\n",
    "\n",
    "    print(f\"Best validation mIoU: {best_miou:.4f}\")\n",
    "    model.load_state_dict(best_state)\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b535894f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"<string>\"\u001b[0m, line \u001b[35m1\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    from multiprocessing.spawn import spawn_main; \u001b[31mspawn_main\u001b[0m\u001b[1;31m(tracker_fd=77, pipe_handle=91)\u001b[0m\n",
      "                                                  \u001b[31m~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/spawn.py\"\u001b[0m, line \u001b[35m122\u001b[0m, in \u001b[35mspawn_main\u001b[0m\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \u001b[35m\"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/spawn.py\"\u001b[0m, line \u001b[35m132\u001b[0m, in \u001b[35m_main\u001b[0m\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "\u001b[1;35mAttributeError\u001b[0m: \u001b[35mCan't get attribute 'SegmentationDataset' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:1\u001b[0m\n",
      "Cell \u001b[0;32mIn[18], line 30\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, checkpoint_dir)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m20\u001b[39m)\n\u001b[0;32m---> 30\u001b[0m train_loss, train_acc, train_time \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m val_loss, val_acc, val_miou, val_per_class_iou \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader, criterion)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m scheduler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[16], line 27\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, dataloader, criterion, optimizer)\u001b[0m\n\u001b[1;32m     24\u001b[0m total_pixels \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     25\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 27\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/utils/data/dataloader.py:493\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    491\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 493\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/utils/data/dataloader.py:424\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[0;32m--> 424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/utils/data/dataloader.py:1171\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1164\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[1;32m   1167\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[1;32m   1168\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[1;32m   1169\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[1;32m   1170\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[0;32m-> 1171\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1172\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/context.py:289\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_posix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, process_obj):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/popen_fork.py:20\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinalizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/popen_spawn_posix.py:62\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentinel \u001b[38;5;241m=\u001b[39m parent_r\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(parent_w, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m, closefd\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 62\u001b[0m         \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetbuffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     fds_to_close \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "if __name__ == \"__main__\":\n",
    "    if len(train_dataset) == 0 or len(val_dataset) == 0:\n",
    "        raise RuntimeError(\"Training/validation datasets are empty. Check the data directory structure.\")\n",
    "\n",
    "    trained_model, history = train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        num_epochs=num_epochs,\n",
    "        scaler=scaler,\n",
    "        use_amp=use_amp,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab10d2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning curves\n",
    "if 'history' in locals():\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 4))\n",
    "\n",
    "    axes[0].plot(epochs, history['train_loss'], label='Train')\n",
    "    axes[0].plot(epochs, history['val_loss'], label='Val')\n",
    "    axes[0].set_title('Loss')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Cross-Entropy')\n",
    "    axes[0].legend()\n",
    "\n",
    "    axes[1].plot(epochs, history['train_pixel_acc'], label='Train')\n",
    "    axes[1].plot(epochs, history['val_pixel_acc'], label='Val')\n",
    "    axes[1].set_title('Pixel Accuracy')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Accuracy')\n",
    "    axes[1].legend()\n",
    "\n",
    "    axes[2].plot(epochs, history['val_mIoU'], label='Val mIoU', color='tab:green')\n",
    "    axes[2].set_title('Validation mIoU')\n",
    "    axes[2].set_xlabel('Epoch')\n",
    "    axes[2].set_ylabel('mIoU')\n",
    "    axes[2].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "else:\n",
    "    print(\"Run the training cell first to generate history.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0c2720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation metrics\n",
    "if 'trained_model' in locals():\n",
    "    val_loss, val_acc, val_miou, val_per_class = evaluate(trained_model, val_loader, criterion, use_amp=use_amp)\n",
    "    print(f\"Validation loss: {val_loss:.4f}\")\n",
    "    print(f\"Validation pixel acc : {val_acc:.4f}\")\n",
    "    print(f\"Validation mIoU      : {val_miou:.4f}\")\n",
    "    for cls_name, cls_iou in zip(classes, val_per_class.tolist()):\n",
    "        print(f\"  IoU({cls_name}): {cls_iou:.4f}\")\n",
    "else:\n",
    "    print(\"Train the model before running this cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9df53e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set evaluation\n",
    "if test_loader is not None and 'trained_model' in locals():\n",
    "    test_loss, test_acc, test_miou, test_per_class = evaluate(trained_model, test_loader, criterion, use_amp=use_amp)\n",
    "    print(f\"Test loss      : {test_loss:.4f}\")\n",
    "    print(f\"Test pixel acc : {test_acc:.4f}\")\n",
    "    print(f\"Test mIoU      : {test_miou:.4f}\")\n",
    "    for cls_name, cls_iou in zip(classes, test_per_class.tolist()):\n",
    "        print(f\"  IoU({cls_name}): {cls_iou:.4f}\")\n",
    "else:\n",
    "    print(\"No test set detected or model has not been trained yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283b1c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qualitative inspection\n",
    "def colorize_mask(mask: torch.Tensor) -> np.ndarray:\n",
    "    palette = np.array([[0, 0, 0], [255, 0, 0]], dtype=np.uint8)\n",
    "    mask_np = mask.cpu().numpy().astype(np.int64)\n",
    "    mask_np = np.clip(mask_np, 0, len(palette) - 1)\n",
    "    return palette[mask_np]\n",
    "\n",
    "if 'trained_model' in locals():\n",
    "    trained_model.eval()\n",
    "    sample_batch = next(iter(val_loader))\n",
    "    images, masks = sample_batch\n",
    "    with torch.no_grad():\n",
    "        outputs = trained_model(images.to(device))[\"out\"]\n",
    "        preds = outputs.argmax(dim=1).cpu()\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    std_tensor = torch.tensor(normalize_std, dtype=images.dtype).view(-1, 1, 1)\n",
    "    mean_tensor = torch.tensor(normalize_mean, dtype=images.dtype).view(-1, 1, 1)\n",
    "\n",
    "    batch_size_vis = min(3, images.size(0))\n",
    "    fig, axes = plt.subplots(batch_size_vis, 3, figsize=(12, 4 * batch_size_vis))\n",
    "    if batch_size_vis == 1:\n",
    "        axes = np.expand_dims(axes, axis=0)\n",
    "    for idx in range(batch_size_vis):\n",
    "        img = images[idx].cpu() * std_tensor + mean_tensor\n",
    "        img = img.permute(1, 2, 0).clamp(0, 1).numpy()\n",
    "        axes[idx, 0].imshow(img)\n",
    "        axes[idx, 0].set_title('Input')\n",
    "        axes[idx, 0].axis('off')\n",
    "\n",
    "        axes[idx, 1].imshow(colorize_mask(masks[idx]))\n",
    "        axes[idx, 1].set_title('Ground Truth')\n",
    "        axes[idx, 1].axis('off')\n",
    "\n",
    "        axes[idx, 2].imshow(colorize_mask(preds[idx]))\n",
    "        axes[idx, 2].set_title('Prediction')\n",
    "        axes[idx, 2].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "else:\n",
    "    print(\"Train the model to visualize qualitative predictions.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
